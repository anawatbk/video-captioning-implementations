{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_PATH = '../data/MSVD_label_final.csv'\n",
    "DATA_PATH = '../data/MSVD/training_data/feat/'\n",
    "MODEL_PATH = '../model/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load utils variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_final_df = pd.read_csv(LABELS_PATH)\n",
    "embedding = np.load(MODEL_PATH+'MSVD_embedding.npy')\n",
    "word2idx = pickle.load(open(MODEL_PATH+'MSVD_word2idx.pkl', \"rb\"))\n",
    "idx2word = pickle.load(open(MODEL_PATH+'MSVD_idx2word.pkl', \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(vid, DATA_PATH):\n",
    "    filename = DATA_PATH+ f'{vid}.npy'\n",
    "    raw_x = torch.tensor(np.load(filename)).float()\n",
    "    return raw_x.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Take sequence of video's resnet50 features as input\n",
    "    \n",
    "    note: batch_first=True does not apply to hidden or cell states\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: PackedSequence\n",
    "        '''\n",
    "        outputs, hidden = self.rnn(x) \n",
    "        return hidden\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    Decode Hidden State from Encoder to sentence (sequence of texts)\n",
    "    \n",
    "    note: batch_first=True does not apply to hidden or cell states\n",
    "    '''\n",
    "    def __init__(self, weights, emb_dim, hidden_dim, out_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        # layers\n",
    "        #self.emb = nn.Embedding.from_pretrained(torch.tensor(weights), padding_idx=0, freeze=False)\n",
    "        self.emb = nn.Embedding(out_dim, emb_dim, padding_idx=0)\n",
    "        self.rnn = nn.GRU(emb_dim + hidden_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.fc_out1 = nn.Linear(emb_dim + hidden_dim * 2, out_dim)\n",
    "        self.fc_out2 = nn.Linear(out_dim, out_dim)\n",
    "\n",
    "                \n",
    "    def forward(self, word_input, encoded_context, hidden):\n",
    "        '''\n",
    "        word_input: (batch_size)\n",
    "        encoded_context: (1, batch_size, hidden_dim)\n",
    "        hidden: (1, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # 1 word at a time\n",
    "    \n",
    "        word_input = self.emb(word_input) # dim (batch, emb_dim) \n",
    "        emb_input = torch.cat([word_input, encoded_context.squeeze(0)], dim=1)\n",
    "        output, hidden = self.rnn(emb_input.unsqueeze(1).float(), hidden)\n",
    "        prediction = F.relu(self.fc_out1(torch.cat([word_input, encoded_context.squeeze(0), hidden.squeeze(0)], dim=1).float()))\n",
    "        prediction = self.fc_out2(prediction)\n",
    "        return prediction, hidden \n",
    "\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, x, y, teacher_forcing_ratio=0.8):\n",
    "        '''\n",
    "        x: PackedSequence\n",
    "        y: (batch_size, sentence_len(padded))\n",
    "        hidden: (1, batch_size, hidden_dim)\n",
    "        '''\n",
    "        batch_size = y.size(0)\n",
    "        sentence_len = y.size(1)\n",
    "        vocab_size = self.decoder.out_dim\n",
    "        \n",
    "        ##############\n",
    "        # Initialize #\n",
    "        ##############\n",
    "        # tensor for final outputs\n",
    "        outputs = torch.zeros(batch_size, sentence_len, vocab_size).to(self.device)\n",
    "        # last hidden state of the encoder is the context\n",
    "        encoded_context = self.encoder(x) # (1, batch_size, hidden_dim)\n",
    "        # first hidden state \n",
    "        hidden = encoded_context # (1, batch_size, hidden_dim)\n",
    "        # first input '<START>'\n",
    "        word_input = y[:, 0] # (batch_size)\n",
    "        for t in range(1, sentence_len):\n",
    "            #insert input token embedding, previous hidden state and the context state\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden = self.decoder(word_input, encoded_context, hidden)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[:, t, :] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = np.random.rand() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) # dim: (batch_size)\n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            word_input = y[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (rnn): GRU(4096, 512, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (emb): Embedding(1504, 300, padding_idx=0)\n",
       "    (rnn): GRU(812, 512, batch_first=True)\n",
       "    (fc_out1): Linear(in_features=1324, out_features=1504, bias=True)\n",
       "    (fc_out2): Linear(in_features=1504, out_features=1504, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_VOCAB = len(word2idx)\n",
    "EMB_DIM = 300\n",
    "INPUT_DIM = 4096 # resnet50 fc dim\n",
    "HIDDEN_DIM = 512\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = Encoder(INPUT_DIM, HIDDEN_DIM)\n",
    "decoder = Decoder(embedding, EMB_DIM, HIDDEN_DIM, N_VOCAB)\n",
    "model = Seq2Seq(encoder, decoder, device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (rnn): GRU(4096, 512, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (emb): Embedding(1504, 300, padding_idx=0)\n",
       "    (rnn): GRU(812, 512, batch_first=True)\n",
       "    (fc_out1): Linear(in_features=1324, out_features=1504, bias=True)\n",
       "    (fc_out2): Linear(in_features=1504, out_features=1504, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(MODEL_PATH+'MSVD_seq2seq_v1.pt', map_location=torch.device('cpu')) )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_seq(model, x, start_token, seq_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        word_input = torch.tensor(word2idx[start_token]).unsqueeze(0)\n",
    "        context = model.encoder(x)\n",
    "        hidden = context\n",
    "        outputs = []\n",
    "        ## generate a sequence!\n",
    "        for i in range(seq_length):\n",
    "            output, hidden = model.decoder(word_input, context, hidden)\n",
    "            word = output.argmax(1)\n",
    "            actual_word = idx2word[word.item()]\n",
    "            if actual_word == '<END>':\n",
    "                return ' '.join(outputs)\n",
    "            outputs.append(actual_word)\n",
    "            word_input = word\n",
    "        return ' '.join(outputs)\n",
    "\n",
    "    \n",
    "def gen_greedy_caption(vid_id, model, DATA_PATH, max_length=10):\n",
    "    context = get_features(vid_id, DATA_PATH)\n",
    "    text = gen_seq(model, context, '<START>', max_length)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "caption     a little girl putting ingredients in a food pr...\n",
       "video_id                                PQbkdRbir0M_45_53.avi\n",
       "sent_len                                                    9\n",
       "Name: 9754, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_final_df.iloc[9754]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a girl is <UNK>'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_id = 'PQbkdRbir0M_45_53.avi'\n",
    "gen_greedy_caption(vid_id, model, DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Gif file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = '../data/MSVD/training_data/video/'\n",
    "SAMPLE_PATH = '../sample/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 0/81 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file ../sample/PQbkdRbir0M_45_53.avi.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "#from tkinter.filedialog import *\n",
    "\n",
    "#video_file = askopenfilename()\n",
    "clip = VideoFileClip(VIDEO_PATH+vid_id)\n",
    "clip.write_gif(SAMPLE_PATH+vid_id+'.gif', fps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
