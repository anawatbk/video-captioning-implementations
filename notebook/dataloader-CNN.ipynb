{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path configuration\n",
    "LABELS_PATH = '../data/train_val_annotation/train_val_videodatainfo.json'\n",
    "DATA_PATH = '../data/train_val_features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON file\n",
    "f = open (LABELS_PATH, \"r\")\n",
    "  # Reading from file\n",
    "data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 9,\n",
       " 'url': 'https://www.youtube.com/watch?v=9lZi22qLlEo',\n",
       " 'video_id': 'video0',\n",
       " 'start time': 137.72,\n",
       " 'end time': 149.44,\n",
       " 'split': 'train',\n",
       " 'id': 0}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['videos'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## randomly select sentence\n",
    "#label_df = pd.DataFrame(data['sentences'])\n",
    "#label_final_df = label_df.groupby('video_id')['sen_id'].unique().apply(lambda x: x[np.random.randint(0,20)]).to_frame().reset_index()\n",
    "#label_final_df['video_id'].nunique()\n",
    "#label_final_df = label_final_df.join(label_df[['sen_id', 'caption']].set_index('sen_id'), on='sen_id')\n",
    "#label_final_df.to_csv('../data/label_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_final_df = pd.read_csv('../data/label_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create embedding matrix from google news word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load('en_core_web_sm', disable = ['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "# embedding\n",
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "glove_emb = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sent = label_final_df['caption'].tolist()#.astype('unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = Counter()\n",
    "try:\n",
    "    for doc in nlp.pipe(all_sent):\n",
    "        for word in doc:\n",
    "            #print(word)\n",
    "            wc[str(word)] += 1\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(doc,'\\nword:', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding matrix\n",
    "# initialization\n",
    "EMBEDDING_SIZE = 300\n",
    "embedding = np.zeros((len(wc)+4, 300)) # +4 for start, end, unk, padding\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "\n",
    "word2idx['<PAD>'] = 0\n",
    "idx2word[0] = '<PAD>'\n",
    "embedding[0] = np.random.rand(300)*2 - 1\n",
    "\n",
    "word2idx['<START>'] = 1\n",
    "idx2word[1] = '<START>'\n",
    "embedding[1] = np.random.rand(300)*2 - 1\n",
    "\n",
    "word2idx['<END>'] = 2\n",
    "idx2word[2] = '<END>'\n",
    "embedding[2] = np.random.rand(300)*2 - 1\n",
    "\n",
    "word2idx['<UNK>'] = 3\n",
    "idx2word[3] = '<UNK>'\n",
    "embedding[3] = np.random.rand(300)*2 - 1\n",
    "\n",
    "count = 0\n",
    "for word, _ in wc.most_common():\n",
    "    wid = len(word2idx)\n",
    "    word2idx[word] = wid\n",
    "    idx2word[wid] = word\n",
    "    if word in glove_emb:\n",
    "        embedding[wid] = glove_emb.get_vector(word)\n",
    "    else:\n",
    "        embedding[wid] = np.random.rand(300)*2 - 1 # random initialisation (-1, 1)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 words are not in google news werd2vec\n"
     ]
    }
   ],
   "source": [
    "print(f'{count} words are not in google news werd2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../model/embedding.npy', embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../model/word2idx.pkl\",\"wb\") as f:\n",
    "    pickle.dump(word2idx, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../model/idx2word.pkl\",\"wb\") as f:\n",
    "    pickle.dump(idx2word, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Dataset Class for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSRVTT(Dataset):\n",
    "    def __init__(self, df, word2idx, DATA_PATH):\n",
    "        super(MSRVTT, self).__init__()\n",
    "        self.df = df\n",
    "        self.path = DATA_PATH\n",
    "        self.word2idx = word2idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        vid = row['video_id']\n",
    "        filename = self.path + f'{vid}-feature.pt5'\n",
    "        x = torch.load(filename)\n",
    "        sentence_emb = [self.word2idx.get(word, self.word2idx['<UNK>']) for word in row['caption'].split(' ')]\n",
    "        y = torch.zeros(len(sentence_emb)+2)\n",
    "        y[0], y[-1] = self.word2idx['<START>'], self.word2idx['<END>']\n",
    "        y[1:-1] = torch.tensor(sentence_emb)\n",
    "        #true_sentence = row['caption']\n",
    "        return x, y.long()#, true_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MSRVTT(label_final_df, word2idx, DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_iter = iter(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_size = []\n",
    "sents_length = []\n",
    "for x, y in ds_iter:\n",
    "    frames_size.append(x.size(0))\n",
    "    sents_length.append(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# frames length distribution\n",
    "plt.hist(frames_size)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQmElEQVR4nO3dcazdZX3H8fdnhSFRiTAupGvLylyXCGSW0HRN2B8oTho1K/5BUpNJ/yCpITXBxGUp/qMuacKSqQuJkNRBKJnaNFFGo7LZdRpnguCFoaUUQiMMrm1o1RjrP2St3/1xnm4n5dB7e+/tudz7vF/Jye/3+/6e5/ye559Pf33O75ybqkKS1IffW+gBSJLGx9CXpI4Y+pLUEUNfkjpi6EtSRy5Y6AFM5/LLL6/Vq1cv9DAkaVF56qmnflFVE2fW3/Khv3r1aiYnJxd6GJK0qCT571F1l3ckqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSR6YN/SRvS/Jkkp8kOZjk861+WZJ9SV5s20uH+tyd5HCSF5LcMlS/IcmBdu7eJDk/05IkjTKTO/3XgfdX1XuBtcDGJBuA7cD+qloD7G/HJLkG2AxcC2wE7kuyrL3X/cBWYE17bZzHuUiSpjFt6NfAb9vhhe1VwCZgV6vvAm5t+5uA3VX1elW9BBwG1idZDlxSVY/X4Ef8Hx7qI0kagxl9I7fdqT8F/Anw5ap6IsmVVXUUoKqOJrmiNV8B/Gio+1Sr/U/bP7M+6npbGfyPgKuuumrms3mLWL392wt27Zfv+fCCXVvSW9+MPsitqlNVtRZYyeCu/bqzNB+1Tl9nqY+63s6qWldV6yYm3vDTEZKkWTqnp3eq6tfA9xmsxb/Wlmxo22Ot2RSwaqjbSuBIq68cUZckjclMnt6ZSPKutn8x8AHgeWAvsKU12wI82vb3ApuTXJTkagYf2D7ZloJOJNnQntq5faiPJGkMZrKmvxzY1db1fw/YU1XfSvI4sCfJHcArwG0AVXUwyR7gOeAksK2qTrX3uhN4CLgYeKy9JEljMm3oV9VPgetH1H8J3PwmfXYAO0bUJ4GzfR4gSTqP/EauJHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1ZEY/rbxYLeRPHEvSW5F3+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHVk2tBPsirJ95IcSnIwyV2t/rkkP0/yTHt9aKjP3UkOJ3khyS1D9RuSHGjn7k2S8zMtSdIoM/kjKieBT1fV00neCTyVZF8796Wq+ofhxkmuATYD1wJ/CPx7kj+tqlPA/cBW4EfAd4CNwGPzMxVJ0nSmvdOvqqNV9XTbPwEcAlacpcsmYHdVvV5VLwGHgfVJlgOXVNXjVVXAw8Ctc56BJGnGzmlNP8lq4HrgiVb6ZJKfJnkwyaWttgJ4dajbVKutaPtn1kddZ2uSySSTx48fP5chSpLOYsahn+QdwDeAT1XVbxgs1bwbWAscBb5wuumI7nWW+huLVTural1VrZuYmJjpECVJ05hR6Ce5kEHgf7WqvglQVa9V1amq+h3wFWB9az4FrBrqvhI40uorR9QlSWMyk6d3AjwAHKqqLw7Vlw81+yjwbNvfC2xOclGSq4E1wJNVdRQ4kWRDe8/bgUfnaR6SpBmYydM7NwIfBw4keabVPgN8LMlaBks0LwOfAKiqg0n2AM8xePJnW3tyB+BO4CHgYgZP7fjkjiSN0bShX1U/ZPR6/HfO0mcHsGNEfRK47lwGKEmaP34jV5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkemDf0kq5J8L8mhJAeT3NXqlyXZl+TFtr10qM/dSQ4neSHJLUP1G5IcaOfuTZLzMy1J0igzudM/CXy6qt4DbAC2JbkG2A7sr6o1wP52TDu3GbgW2Ajcl2RZe6/7ga3AmvbaOI9zkSRNY9rQr6qjVfV02z8BHAJWAJuAXa3ZLuDWtr8J2F1Vr1fVS8BhYH2S5cAlVfV4VRXw8FAfSdIYnNOafpLVwPXAE8CVVXUUBv8wAFe0ZiuAV4e6TbXairZ/Zn3UdbYmmUwyefz48XMZoiTpLGYc+kneAXwD+FRV/eZsTUfU6iz1NxardlbVuqpaNzExMdMhSpKmMaPQT3Ihg8D/alV9s5Vfa0s2tO2xVp8CVg11XwkcafWVI+qSpDGZydM7AR4ADlXVF4dO7QW2tP0twKND9c1JLkpyNYMPbJ9sS0Ankmxo73n7UB9J0hhcMIM2NwIfBw4keabVPgPcA+xJcgfwCnAbQFUdTLIHeI7Bkz/bqupU63cn8BBwMfBYe0mSxmTa0K+qHzJ6PR7g5jfpswPYMaI+CVx3LgOUJM0fv5ErSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR2ZNvSTPJjkWJJnh2qfS/LzJM+014eGzt2d5HCSF5LcMlS/IcmBdu7eJJn/6UiSzmYmd/oPARtH1L9UVWvb6zsASa4BNgPXtj73JVnW2t8PbAXWtNeo95QknUfThn5V/QD41QzfbxOwu6per6qXgMPA+iTLgUuq6vGqKuBh4NbZDlqSNDtzWdP/ZJKftuWfS1ttBfDqUJupVlvR9s+sj5Rka5LJJJPHjx+fwxAlScNmG/r3A+8G1gJHgS+0+qh1+jpLfaSq2llV66pq3cTExCyHKEk606xCv6peq6pTVfU74CvA+nZqClg11HQlcKTVV46oS5LGaFah39boT/socPrJnr3A5iQXJbmawQe2T1bVUeBEkg3tqZ3bgUfnMG5J0ixcMF2DJF8HbgIuTzIFfBa4KclaBks0LwOfAKiqg0n2AM8BJ4FtVXWqvdWdDJ4Euhh4rL0kSWM0behX1cdGlB84S/sdwI4R9UngunManSRpXvmNXEnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR2ZNvSTPJjkWJJnh2qXJdmX5MW2vXTo3N1JDid5IcktQ/Ubkhxo5+5NkvmfjiTpbGZyp/8QsPGM2nZgf1WtAfa3Y5JcA2wGrm197kuyrPW5H9gKrGmvM99TknSeTRv6VfUD4FdnlDcBu9r+LuDWofruqnq9ql4CDgPrkywHLqmqx6uqgIeH+kiSxmS2a/pXVtVRgLa9otVXAK8OtZtqtRVt/8z6SEm2JplMMnn8+PFZDlGSdKb5/iB31Dp9naU+UlXtrKp1VbVuYmJi3gYnSb2bbei/1pZsaNtjrT4FrBpqtxI40uorR9QlSWM029DfC2xp+1uAR4fqm5NclORqBh/YPtmWgE4k2dCe2rl9qI8kaUwumK5Bkq8DNwGXJ5kCPgvcA+xJcgfwCnAbQFUdTLIHeA44CWyrqlPtre5k8CTQxcBj7SVJGqNpQ7+qPvYmp25+k/Y7gB0j6pPAdec0OknSvPIbuZLUEUNfkjoy7fKOFpfV27+9INd9+Z4PL8h1JZ0b7/QlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JH5hT6SV5OciDJM0kmW+2yJPuSvNi2lw61vzvJ4SQvJLllroOXJJ2b+bjTf19Vra2qde14O7C/qtYA+9sxSa4BNgPXAhuB+5Ism4frS5Jm6Hws72wCdrX9XcCtQ/XdVfV6Vb0EHAbWn4frS5LexFxDv4DvJnkqydZWu7KqjgK07RWtvgJ4dajvVKu9QZKtSSaTTB4/fnyOQ5QknXbBHPvfWFVHklwB7Evy/FnaZkStRjWsqp3AToB169aNbCNJOndzutOvqiNtewx4hMFyzWtJlgO07bHWfApYNdR9JXBkLteXJJ2bWYd+krcneefpfeCDwLPAXmBLa7YFeLTt7wU2J7koydXAGuDJ2V5fknTu5rK8cyXwSJLT7/O1qvrXJD8G9iS5A3gFuA2gqg4m2QM8B5wEtlXVqTmNXpJ0TmYd+lX1M+C9I+q/BG5+kz47gB2zvaYkaW78Rq4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOjLXP6IiAbB6+7cX7Nov3/PhBbu2tNh4py9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE397RordQv/vjb/5oMfJOX5I6MvbQT7IxyQtJDifZPu7rS1LPxrq8k2QZ8GXgL4Ep4MdJ9lbVc+MchzQf/DlpLUbjXtNfDxyuqp8BJNkNbAIMfekc+DmGZmvcob8CeHXoeAr48zMbJdkKbG2Hv03ywhjGNt8uB36x0INYAM57Ccvfv6HUxbxHWAzz/qNRxXGHfkbU6g2Fqp3AzvM/nPMnyWRVrVvocYyb8+6L8158xv1B7hSwauh4JXBkzGOQpG6NO/R/DKxJcnWS3wc2A3vHPAZJ6tZYl3eq6mSSTwL/BiwDHqyqg+Mcwxgt6uWpOXDefXHei0yq3rCkLklaovxGriR1xNCXpI4Y+vMgyYNJjiV5dqh2WZJ9SV5s20sXcozzLcmqJN9LcijJwSR3tfqSnjdAkrcleTLJT9rcP9/qPcx9WZL/SvKtdrzk5wyQ5OUkB5I8k2Sy1Rbl3A39+fEQsPGM2nZgf1WtAfa346XkJPDpqnoPsAHYluQalv68AV4H3l9V7wXWAhuTbKCPud8FHBo67mHOp72vqtYOPZ+/KOdu6M+DqvoB8KszypuAXW1/F3DrWAd1nlXV0ap6uu2fYBAEK1ji8waogd+2wwvbq1jic0+yEvgw8E9D5SU952ksyrkb+ufPlVV1FAYBCVyxwOM5b5KsBq4HnqCTebdljmeAY8C+quph7v8I/C3wu6HaUp/zaQV8N8lT7WdiYJHO3T+iojlJ8g7gG8Cnquo3yahf2lh6quoUsDbJu4BHkly30GM6n5J8BDhWVU8luWmhx7MAbqyqI0muAPYleX6hBzRb3umfP68lWQ7QtscWeDzzLsmFDAL/q1X1zVZe8vMeVlW/Br7P4DOdpTz3G4G/SvIysBt4f5J/ZmnP+f9U1ZG2PQY8wuAXgxfl3A3982cvsKXtbwEeXcCxzLsMbukfAA5V1ReHTi3peQMkmWh3+CS5GPgA8DxLeO5VdXdVrayq1Qx+PuU/quqvWcJzPi3J25O88/Q+8EHgWRbp3P1G7jxI8nXgJgY/t/oa8FngX4A9wFXAK8BtVXXmh72LVpK/AP4TOMD/r/F+hsG6/pKdN0CSP2Pwwd0yBjdOe6rq75L8AUt87gBteedvquojPcw5yR8zuLuHwZL416pqx2Kdu6EvSR1xeUeSOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI78LwBh2d8jEnl6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sentence length distribution\n",
    "plt.hist(sents_length)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    padding  text or video\n",
    "    \"\"\"\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    for x, y in batch:\n",
    "        x_batch.append(x)\n",
    "        y_batch.append(y)\n",
    "    lengths = list(map(lambda x: x.size(0), x_batch))\n",
    "    x_batch = torch.nn.utils.rnn.pad_sequence(x_batch,  batch_first=True)\n",
    "    x_batch = torch.nn.utils.rnn.pack_padded_sequence(x_batch, lengths, batch_first=True, enforce_sorted=False)\n",
    "    y_batch = torch.nn.utils.rnn.pad_sequence(y_batch, batch_first=True)\n",
    "    return x_batch, y_batch#, lengths  # PackedSequence, padded tensor\n",
    "\n",
    "#torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6519 491\n"
     ]
    }
   ],
   "source": [
    "# split train/test\n",
    "train_proportion = 0.93\n",
    "train_size = int(train_proportion * len(ds))\n",
    "validation_size = len(ds) - train_size\n",
    "print(train_size, validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, validation_ds = random_split(ds, [train_size, validation_size])\n",
    "# dataloaders\n",
    "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(validation_ds, batch_size=2, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([370, 2048])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to Sequence Model (Encoder-Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While image description handles a variable length output sequence of words, video description also has to handle a variable length input sequence of frames. Related approaches to video description have resolved variable length input by holistic video representations [29, 28, 11], pooling over frames [39], or sub-sampling on a fixed number of input frames [43]. In contrast, in this work we propose a sequence to sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions for the input data:\n",
    "1. I tried the following code with x.data as well, but the it losts information for the batch size, in this case it is 8.\n",
    "2. The input dim is 2, while conv2d in the pretrained model neeeds data to be 4-dim, even conv1d needs dim-3 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models import resnet18, resnet101\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.densenet = models.densenet121(pretrained=True)\n",
    "        \n",
    "        self.densenet.classifier = nn.Linear(in_features=1024, out_features=1024)\n",
    "        \n",
    "        # add another fully connected layer\n",
    "        self.embed = nn.Linear(in_features=1024, out_features=hidden_dim)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # activation layers\n",
    "        self.prelu = nn.PReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # get the embeddings from the densenet\n",
    "        densenet_outputs = self.dropout(self.prelu(self.densenet(x)))\n",
    "        \n",
    "        # pass through the fully connected\n",
    "        embeddings = self.embed(densenet_outputs)\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    Decode Hidden State from Encoder to sentence (sequence of texts)\n",
    "    \n",
    "    note: batch_first=True does not apply to hidden or cell states\n",
    "    '''\n",
    "    def __init__(self, weights, emb_dim, hidden_dim, out_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        # layers\n",
    "        self.emb = nn.Embedding.from_pretrained(torch.tensor(weights), padding_idx=0, freeze=False)\n",
    "        self.rnn = nn.GRU(emb_dim + hidden_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.fc_out = nn.Linear(emb_dim + hidden_dim * 2, out_dim)\n",
    "                \n",
    "    def forward(self, word_input, encoded_context, hidden):\n",
    "        '''\n",
    "        word_input: (batch_size)\n",
    "        encoded_context: (1, batch_size, hidden_dim)\n",
    "        hidden: (1, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # 1 word at a time\n",
    "    \n",
    "        word_input = self.emb(word_input) # dim (batch, emb_dim)\n",
    "        print(word_input.size())\n",
    "        print(encoded_context.size())\n",
    "        emb_input = torch.cat([word_input, encoded_context.squeeze(0)], dim=1)\n",
    "        output, hidden = self.rnn(emb_input.unsqueeze(1).float(), hidden)\n",
    "        prediction = self.fc_out(torch.cat([word_input, encoded_context.squeeze(0), hidden.squeeze(0)], dim=1).float())\n",
    "        return prediction, hidden \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, x, y, teacher_forcing_ratio=0.8):\n",
    "        '''\n",
    "        x: PackedSequence\n",
    "        y: (batch_size, sentence_len(padded))\n",
    "        hidden: (1, batch_size, hidden_dim)\n",
    "        '''\n",
    "        batch_size = y.size(0)\n",
    "        sentence_len = y.size(1)\n",
    "        vocab_size = self.decoder.out_dim\n",
    "        \n",
    "        ##############\n",
    "        # Initialize #\n",
    "        ##############\n",
    "        # tensor for final outputs\n",
    "        outputs = torch.zeros(batch_size, sentence_len, vocab_size).to(self.device)\n",
    "        # last hidden state of the encoder is the context\n",
    "        encoded_context = self.encoder(x) # (1, batch_size, hidden_dim)\n",
    "        # first hidden state \n",
    "        hidden = encoded_context # (1, batch_size, hidden_dim)\n",
    "        # first input '<START>'\n",
    "        word_input = y[:, 0] # (batch_size)\n",
    "        for t in range(1, sentence_len):\n",
    "            #insert input token embedding, previous hidden state and the context state\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden = self.decoder(word_input, encoded_context, hidden)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[:, t, :] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = np.random.rand() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) # dim: (batch_size)\n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            word_input = y[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VOCAB = len(word2idx)\n",
    "EMB_DIM = 300\n",
    "INPUT_DIM = 2048 # resnet50 fc dim\n",
    "HIDDEN_DIM = 800\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = Encoder(INPUT_DIM, HIDDEN_DIM)\n",
    "decoder = Decoder(embedding, EMB_DIM, HIDDEN_DIM, N_VOCAB)\n",
    "model = Seq2Seq(encoder, decoder, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight initialization with N(0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.densenet.features.conv0.weight torch.Size([64, 3, 7, 7])\n",
      "encoder.densenet.features.norm0.weight torch.Size([64])\n",
      "encoder.densenet.features.norm0.bias torch.Size([64])\n",
      "encoder.densenet.features.denseblock1.denselayer1.norm1.weight torch.Size([64])\n",
      "encoder.densenet.features.denseblock1.denselayer1.norm1.bias torch.Size([64])\n",
      "encoder.densenet.features.denseblock1.denselayer1.conv1.weight torch.Size([128, 64, 1, 1])\n",
      "encoder.densenet.features.denseblock1.denselayer1.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock1.denselayer1.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock1.denselayer1.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock1.denselayer2.norm1.weight torch.Size([96])\n",
      "encoder.densenet.features.denseblock1.denselayer2.norm1.bias torch.Size([96])\n",
      "encoder.densenet.features.denseblock1.denselayer2.conv1.weight torch.Size([128, 96, 1, 1])\n",
      "encoder.densenet.features.denseblock1.denselayer2.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock1.denselayer2.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock1.denselayer2.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock1.denselayer3.norm1.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock1.denselayer3.norm1.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock1.denselayer3.conv1.weight torch.Size([128, 128, 1, 1])\n",
      "encoder.densenet.features.denseblock1.denselayer3.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock1.denselayer3.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock1.denselayer3.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock1.denselayer4.norm1.weight torch.Size([160])\n",
      "encoder.densenet.features.denseblock1.denselayer4.norm1.bias torch.Size([160])\n",
      "encoder.densenet.features.denseblock1.denselayer4.conv1.weight torch.Size([128, 160, 1, 1])\n",
      "encoder.densenet.features.denseblock1.denselayer4.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock1.denselayer4.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock1.denselayer4.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock1.denselayer5.norm1.weight torch.Size([192])\n",
      "encoder.densenet.features.denseblock1.denselayer5.norm1.bias torch.Size([192])\n",
      "encoder.densenet.features.denseblock1.denselayer5.conv1.weight torch.Size([128, 192, 1, 1])\n",
      "encoder.densenet.features.denseblock1.denselayer5.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock1.denselayer5.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock1.denselayer5.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock1.denselayer6.norm1.weight torch.Size([224])\n",
      "encoder.densenet.features.denseblock1.denselayer6.norm1.bias torch.Size([224])\n",
      "encoder.densenet.features.denseblock1.denselayer6.conv1.weight torch.Size([128, 224, 1, 1])\n",
      "encoder.densenet.features.denseblock1.denselayer6.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock1.denselayer6.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock1.denselayer6.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.transition1.norm.weight torch.Size([256])\n",
      "encoder.densenet.features.transition1.norm.bias torch.Size([256])\n",
      "encoder.densenet.features.transition1.conv.weight torch.Size([128, 256, 1, 1])\n",
      "encoder.densenet.features.denseblock2.denselayer1.norm1.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer1.norm1.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer1.conv1.weight torch.Size([128, 128, 1, 1])\n",
      "encoder.densenet.features.denseblock2.denselayer1.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer1.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer1.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock2.denselayer2.norm1.weight torch.Size([160])\n",
      "encoder.densenet.features.denseblock2.denselayer2.norm1.bias torch.Size([160])\n",
      "encoder.densenet.features.denseblock2.denselayer2.conv1.weight torch.Size([128, 160, 1, 1])\n",
      "encoder.densenet.features.denseblock2.denselayer2.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer2.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer2.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock2.denselayer3.norm1.weight torch.Size([192])\n",
      "encoder.densenet.features.denseblock2.denselayer3.norm1.bias torch.Size([192])\n",
      "encoder.densenet.features.denseblock2.denselayer3.conv1.weight torch.Size([128, 192, 1, 1])\n",
      "encoder.densenet.features.denseblock2.denselayer3.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer3.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer3.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock2.denselayer4.norm1.weight torch.Size([224])\n",
      "encoder.densenet.features.denseblock2.denselayer4.norm1.bias torch.Size([224])\n",
      "encoder.densenet.features.denseblock2.denselayer4.conv1.weight torch.Size([128, 224, 1, 1])\n",
      "encoder.densenet.features.denseblock2.denselayer4.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer4.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer4.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock2.denselayer5.norm1.weight torch.Size([256])\n",
      "encoder.densenet.features.denseblock2.denselayer5.norm1.bias torch.Size([256])\n",
      "encoder.densenet.features.denseblock2.denselayer5.conv1.weight torch.Size([128, 256, 1, 1])\n",
      "encoder.densenet.features.denseblock2.denselayer5.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer5.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer5.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock2.denselayer6.norm1.weight torch.Size([288])\n",
      "encoder.densenet.features.denseblock2.denselayer6.norm1.bias torch.Size([288])\n",
      "encoder.densenet.features.denseblock2.denselayer6.conv1.weight torch.Size([128, 288, 1, 1])\n",
      "encoder.densenet.features.denseblock2.denselayer6.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer6.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer6.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock2.denselayer7.norm1.weight torch.Size([320])\n",
      "encoder.densenet.features.denseblock2.denselayer7.norm1.bias torch.Size([320])\n",
      "encoder.densenet.features.denseblock2.denselayer7.conv1.weight torch.Size([128, 320, 1, 1])\n",
      "encoder.densenet.features.denseblock2.denselayer7.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer7.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer7.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock2.denselayer8.norm1.weight torch.Size([352])\n",
      "encoder.densenet.features.denseblock2.denselayer8.norm1.bias torch.Size([352])\n",
      "encoder.densenet.features.denseblock2.denselayer8.conv1.weight torch.Size([128, 352, 1, 1])\n",
      "encoder.densenet.features.denseblock2.denselayer8.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer8.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer8.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock2.denselayer9.norm1.weight torch.Size([384])\n",
      "encoder.densenet.features.denseblock2.denselayer9.norm1.bias torch.Size([384])\n",
      "encoder.densenet.features.denseblock2.denselayer9.conv1.weight torch.Size([128, 384, 1, 1])\n",
      "encoder.densenet.features.denseblock2.denselayer9.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer9.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer9.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock2.denselayer10.norm1.weight torch.Size([416])\n",
      "encoder.densenet.features.denseblock2.denselayer10.norm1.bias torch.Size([416])\n",
      "encoder.densenet.features.denseblock2.denselayer10.conv1.weight torch.Size([128, 416, 1, 1])\n",
      "encoder.densenet.features.denseblock2.denselayer10.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer10.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer10.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock2.denselayer11.norm1.weight torch.Size([448])\n",
      "encoder.densenet.features.denseblock2.denselayer11.norm1.bias torch.Size([448])\n",
      "encoder.densenet.features.denseblock2.denselayer11.conv1.weight torch.Size([128, 448, 1, 1])\n",
      "encoder.densenet.features.denseblock2.denselayer11.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer11.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer11.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock2.denselayer12.norm1.weight torch.Size([480])\n",
      "encoder.densenet.features.denseblock2.denselayer12.norm1.bias torch.Size([480])\n",
      "encoder.densenet.features.denseblock2.denselayer12.conv1.weight torch.Size([128, 480, 1, 1])\n",
      "encoder.densenet.features.denseblock2.denselayer12.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer12.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock2.denselayer12.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.transition2.norm.weight torch.Size([512])\n",
      "encoder.densenet.features.transition2.norm.bias torch.Size([512])\n",
      "encoder.densenet.features.transition2.conv.weight torch.Size([256, 512, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer1.norm1.weight torch.Size([256])\n",
      "encoder.densenet.features.denseblock3.denselayer1.norm1.bias torch.Size([256])\n",
      "encoder.densenet.features.denseblock3.denselayer1.conv1.weight torch.Size([128, 256, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer1.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer1.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer1.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer2.norm1.weight torch.Size([288])\n",
      "encoder.densenet.features.denseblock3.denselayer2.norm1.bias torch.Size([288])\n",
      "encoder.densenet.features.denseblock3.denselayer2.conv1.weight torch.Size([128, 288, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer2.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer2.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer2.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer3.norm1.weight torch.Size([320])\n",
      "encoder.densenet.features.denseblock3.denselayer3.norm1.bias torch.Size([320])\n",
      "encoder.densenet.features.denseblock3.denselayer3.conv1.weight torch.Size([128, 320, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer3.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer3.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer3.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer4.norm1.weight torch.Size([352])\n",
      "encoder.densenet.features.denseblock3.denselayer4.norm1.bias torch.Size([352])\n",
      "encoder.densenet.features.denseblock3.denselayer4.conv1.weight torch.Size([128, 352, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer4.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer4.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer4.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer5.norm1.weight torch.Size([384])\n",
      "encoder.densenet.features.denseblock3.denselayer5.norm1.bias torch.Size([384])\n",
      "encoder.densenet.features.denseblock3.denselayer5.conv1.weight torch.Size([128, 384, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer5.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer5.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer5.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer6.norm1.weight torch.Size([416])\n",
      "encoder.densenet.features.denseblock3.denselayer6.norm1.bias torch.Size([416])\n",
      "encoder.densenet.features.denseblock3.denselayer6.conv1.weight torch.Size([128, 416, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer6.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer6.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer6.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer7.norm1.weight torch.Size([448])\n",
      "encoder.densenet.features.denseblock3.denselayer7.norm1.bias torch.Size([448])\n",
      "encoder.densenet.features.denseblock3.denselayer7.conv1.weight torch.Size([128, 448, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer7.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer7.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer7.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer8.norm1.weight torch.Size([480])\n",
      "encoder.densenet.features.denseblock3.denselayer8.norm1.bias torch.Size([480])\n",
      "encoder.densenet.features.denseblock3.denselayer8.conv1.weight torch.Size([128, 480, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer8.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer8.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer8.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer9.norm1.weight torch.Size([512])\n",
      "encoder.densenet.features.denseblock3.denselayer9.norm1.bias torch.Size([512])\n",
      "encoder.densenet.features.denseblock3.denselayer9.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer9.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer9.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer9.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer10.norm1.weight torch.Size([544])\n",
      "encoder.densenet.features.denseblock3.denselayer10.norm1.bias torch.Size([544])\n",
      "encoder.densenet.features.denseblock3.denselayer10.conv1.weight torch.Size([128, 544, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer10.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer10.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer10.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer11.norm1.weight torch.Size([576])\n",
      "encoder.densenet.features.denseblock3.denselayer11.norm1.bias torch.Size([576])\n",
      "encoder.densenet.features.denseblock3.denselayer11.conv1.weight torch.Size([128, 576, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer11.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer11.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer11.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer12.norm1.weight torch.Size([608])\n",
      "encoder.densenet.features.denseblock3.denselayer12.norm1.bias torch.Size([608])\n",
      "encoder.densenet.features.denseblock3.denselayer12.conv1.weight torch.Size([128, 608, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer12.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer12.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer12.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer13.norm1.weight torch.Size([640])\n",
      "encoder.densenet.features.denseblock3.denselayer13.norm1.bias torch.Size([640])\n",
      "encoder.densenet.features.denseblock3.denselayer13.conv1.weight torch.Size([128, 640, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer13.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer13.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer13.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer14.norm1.weight torch.Size([672])\n",
      "encoder.densenet.features.denseblock3.denselayer14.norm1.bias torch.Size([672])\n",
      "encoder.densenet.features.denseblock3.denselayer14.conv1.weight torch.Size([128, 672, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer14.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer14.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer14.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer15.norm1.weight torch.Size([704])\n",
      "encoder.densenet.features.denseblock3.denselayer15.norm1.bias torch.Size([704])\n",
      "encoder.densenet.features.denseblock3.denselayer15.conv1.weight torch.Size([128, 704, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer15.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer15.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer15.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer16.norm1.weight torch.Size([736])\n",
      "encoder.densenet.features.denseblock3.denselayer16.norm1.bias torch.Size([736])\n",
      "encoder.densenet.features.denseblock3.denselayer16.conv1.weight torch.Size([128, 736, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer16.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer16.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer16.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer17.norm1.weight torch.Size([768])\n",
      "encoder.densenet.features.denseblock3.denselayer17.norm1.bias torch.Size([768])\n",
      "encoder.densenet.features.denseblock3.denselayer17.conv1.weight torch.Size([128, 768, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer17.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer17.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer17.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer18.norm1.weight torch.Size([800])\n",
      "encoder.densenet.features.denseblock3.denselayer18.norm1.bias torch.Size([800])\n",
      "encoder.densenet.features.denseblock3.denselayer18.conv1.weight torch.Size([128, 800, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer18.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer18.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer18.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer19.norm1.weight torch.Size([832])\n",
      "encoder.densenet.features.denseblock3.denselayer19.norm1.bias torch.Size([832])\n",
      "encoder.densenet.features.denseblock3.denselayer19.conv1.weight torch.Size([128, 832, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer19.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer19.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer19.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer20.norm1.weight torch.Size([864])\n",
      "encoder.densenet.features.denseblock3.denselayer20.norm1.bias torch.Size([864])\n",
      "encoder.densenet.features.denseblock3.denselayer20.conv1.weight torch.Size([128, 864, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer20.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer20.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer20.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer21.norm1.weight torch.Size([896])\n",
      "encoder.densenet.features.denseblock3.denselayer21.norm1.bias torch.Size([896])\n",
      "encoder.densenet.features.denseblock3.denselayer21.conv1.weight torch.Size([128, 896, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer21.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer21.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer21.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer22.norm1.weight torch.Size([928])\n",
      "encoder.densenet.features.denseblock3.denselayer22.norm1.bias torch.Size([928])\n",
      "encoder.densenet.features.denseblock3.denselayer22.conv1.weight torch.Size([128, 928, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer22.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer22.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer22.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer23.norm1.weight torch.Size([960])\n",
      "encoder.densenet.features.denseblock3.denselayer23.norm1.bias torch.Size([960])\n",
      "encoder.densenet.features.denseblock3.denselayer23.conv1.weight torch.Size([128, 960, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer23.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer23.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer23.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock3.denselayer24.norm1.weight torch.Size([992])\n",
      "encoder.densenet.features.denseblock3.denselayer24.norm1.bias torch.Size([992])\n",
      "encoder.densenet.features.denseblock3.denselayer24.conv1.weight torch.Size([128, 992, 1, 1])\n",
      "encoder.densenet.features.denseblock3.denselayer24.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer24.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock3.denselayer24.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.transition3.norm.weight torch.Size([1024])\n",
      "encoder.densenet.features.transition3.norm.bias torch.Size([1024])\n",
      "encoder.densenet.features.transition3.conv.weight torch.Size([512, 1024, 1, 1])\n",
      "encoder.densenet.features.denseblock4.denselayer1.norm1.weight torch.Size([512])\n",
      "encoder.densenet.features.denseblock4.denselayer1.norm1.bias torch.Size([512])\n",
      "encoder.densenet.features.denseblock4.denselayer1.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "encoder.densenet.features.denseblock4.denselayer1.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer1.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer1.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock4.denselayer2.norm1.weight torch.Size([544])\n",
      "encoder.densenet.features.denseblock4.denselayer2.norm1.bias torch.Size([544])\n",
      "encoder.densenet.features.denseblock4.denselayer2.conv1.weight torch.Size([128, 544, 1, 1])\n",
      "encoder.densenet.features.denseblock4.denselayer2.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer2.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer2.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock4.denselayer3.norm1.weight torch.Size([576])\n",
      "encoder.densenet.features.denseblock4.denselayer3.norm1.bias torch.Size([576])\n",
      "encoder.densenet.features.denseblock4.denselayer3.conv1.weight torch.Size([128, 576, 1, 1])\n",
      "encoder.densenet.features.denseblock4.denselayer3.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer3.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer3.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock4.denselayer4.norm1.weight torch.Size([608])\n",
      "encoder.densenet.features.denseblock4.denselayer4.norm1.bias torch.Size([608])\n",
      "encoder.densenet.features.denseblock4.denselayer4.conv1.weight torch.Size([128, 608, 1, 1])\n",
      "encoder.densenet.features.denseblock4.denselayer4.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer4.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer4.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock4.denselayer5.norm1.weight torch.Size([640])\n",
      "encoder.densenet.features.denseblock4.denselayer5.norm1.bias torch.Size([640])\n",
      "encoder.densenet.features.denseblock4.denselayer5.conv1.weight torch.Size([128, 640, 1, 1])\n",
      "encoder.densenet.features.denseblock4.denselayer5.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer5.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer5.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock4.denselayer6.norm1.weight torch.Size([672])\n",
      "encoder.densenet.features.denseblock4.denselayer6.norm1.bias torch.Size([672])\n",
      "encoder.densenet.features.denseblock4.denselayer6.conv1.weight torch.Size([128, 672, 1, 1])\n",
      "encoder.densenet.features.denseblock4.denselayer6.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer6.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer6.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock4.denselayer7.norm1.weight torch.Size([704])\n",
      "encoder.densenet.features.denseblock4.denselayer7.norm1.bias torch.Size([704])\n",
      "encoder.densenet.features.denseblock4.denselayer7.conv1.weight torch.Size([128, 704, 1, 1])\n",
      "encoder.densenet.features.denseblock4.denselayer7.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer7.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer7.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock4.denselayer8.norm1.weight torch.Size([736])\n",
      "encoder.densenet.features.denseblock4.denselayer8.norm1.bias torch.Size([736])\n",
      "encoder.densenet.features.denseblock4.denselayer8.conv1.weight torch.Size([128, 736, 1, 1])\n",
      "encoder.densenet.features.denseblock4.denselayer8.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer8.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer8.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock4.denselayer9.norm1.weight torch.Size([768])\n",
      "encoder.densenet.features.denseblock4.denselayer9.norm1.bias torch.Size([768])\n",
      "encoder.densenet.features.denseblock4.denselayer9.conv1.weight torch.Size([128, 768, 1, 1])\n",
      "encoder.densenet.features.denseblock4.denselayer9.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer9.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer9.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock4.denselayer10.norm1.weight torch.Size([800])\n",
      "encoder.densenet.features.denseblock4.denselayer10.norm1.bias torch.Size([800])\n",
      "encoder.densenet.features.denseblock4.denselayer10.conv1.weight torch.Size([128, 800, 1, 1])\n",
      "encoder.densenet.features.denseblock4.denselayer10.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer10.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer10.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock4.denselayer11.norm1.weight torch.Size([832])\n",
      "encoder.densenet.features.denseblock4.denselayer11.norm1.bias torch.Size([832])\n",
      "encoder.densenet.features.denseblock4.denselayer11.conv1.weight torch.Size([128, 832, 1, 1])\n",
      "encoder.densenet.features.denseblock4.denselayer11.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer11.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer11.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock4.denselayer12.norm1.weight torch.Size([864])\n",
      "encoder.densenet.features.denseblock4.denselayer12.norm1.bias torch.Size([864])\n",
      "encoder.densenet.features.denseblock4.denselayer12.conv1.weight torch.Size([128, 864, 1, 1])\n",
      "encoder.densenet.features.denseblock4.denselayer12.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer12.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer12.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock4.denselayer13.norm1.weight torch.Size([896])\n",
      "encoder.densenet.features.denseblock4.denselayer13.norm1.bias torch.Size([896])\n",
      "encoder.densenet.features.denseblock4.denselayer13.conv1.weight torch.Size([128, 896, 1, 1])\n",
      "encoder.densenet.features.denseblock4.denselayer13.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer13.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer13.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock4.denselayer14.norm1.weight torch.Size([928])\n",
      "encoder.densenet.features.denseblock4.denselayer14.norm1.bias torch.Size([928])\n",
      "encoder.densenet.features.denseblock4.denselayer14.conv1.weight torch.Size([128, 928, 1, 1])\n",
      "encoder.densenet.features.denseblock4.denselayer14.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer14.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer14.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock4.denselayer15.norm1.weight torch.Size([960])\n",
      "encoder.densenet.features.denseblock4.denselayer15.norm1.bias torch.Size([960])\n",
      "encoder.densenet.features.denseblock4.denselayer15.conv1.weight torch.Size([128, 960, 1, 1])\n",
      "encoder.densenet.features.denseblock4.denselayer15.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer15.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer15.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.denseblock4.denselayer16.norm1.weight torch.Size([992])\n",
      "encoder.densenet.features.denseblock4.denselayer16.norm1.bias torch.Size([992])\n",
      "encoder.densenet.features.denseblock4.denselayer16.conv1.weight torch.Size([128, 992, 1, 1])\n",
      "encoder.densenet.features.denseblock4.denselayer16.norm2.weight torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer16.norm2.bias torch.Size([128])\n",
      "encoder.densenet.features.denseblock4.denselayer16.conv2.weight torch.Size([32, 128, 3, 3])\n",
      "encoder.densenet.features.norm5.weight torch.Size([1024])\n",
      "encoder.densenet.features.norm5.bias torch.Size([1024])\n",
      "encoder.densenet.classifier.weight torch.Size([1024, 1024])\n",
      "encoder.densenet.classifier.bias torch.Size([1024])\n",
      "encoder.embed.weight torch.Size([800, 1024])\n",
      "encoder.embed.bias torch.Size([800])\n",
      "encoder.prelu.weight torch.Size([1])\n",
      "decoder.rnn.weight_ih_l0 torch.Size([2400, 1100])\n",
      "decoder.rnn.weight_hh_l0 torch.Size([2400, 800])\n",
      "decoder.rnn.bias_ih_l0 torch.Size([2400])\n",
      "decoder.rnn.bias_hh_l0 torch.Size([2400])\n",
      "decoder.fc_out.weight torch.Size([6157, 1900])\n",
      "decoder.fc_out.bias torch.Size([6157])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name != 'decoder.emb.weight':\n",
    "        print(name, param.shape)\n",
    "        nn.init.normal_(param.data, mean=0, std=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=0)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, threshold=0.001, threshold_mode='rel', min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, lossFun, backwards=True, print_loss=False):\n",
    "    \n",
    "    if backwards == True:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for i, (x, y) in enumerate(tqdm(dataloader)):\n",
    "        \n",
    "        out = model(x, y)\n",
    "        out = out.view(-1, 6157)\n",
    "        y = y.view(-1)\n",
    "        loss = lossFun(out, y)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if i % 50 == 0:\n",
    "            print(f'Batch loss: {loss.item()}')\n",
    "        if backwards == True:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    if print_loss == True:\n",
    "        print(avg_loss)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, lossFun):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for x, y in tqdm(dataloader):\n",
    "\n",
    "            out = model(x, y)\n",
    "            out = out.view(-1, 6157)\n",
    "            y = y.view(-1)\n",
    "            loss = lossFun(out, y)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/815 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (PackedSequence, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mPackedSequence\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mNoneType\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mPackedSequence\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mNoneType\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-ed150cd44f8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Loss: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-205-bd307d21c942>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, lossFun, backwards, print_loss)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6157\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-201-4d60f39e9cea>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# last hidden state of the encoder is the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mencoded_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (1, batch_size, hidden_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m# first hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_context\u001b[0m \u001b[0;31m# (1, batch_size, hidden_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-200-218ba7f5a56e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# get the embeddings from the densenet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mdensenet_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdensenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# pass through the fully connected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (PackedSequence, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mPackedSequence\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mNoneType\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mPackedSequence\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mNoneType\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "# train 3 more epochs\n",
    "EPOCHS = 5\n",
    "best = 100000\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('Epoch: ', epoch)\n",
    "    \n",
    "    train_loss = train_one_epoch(model, train_dl, optimizer, loss_func)\n",
    "    print('Train Loss: ', train_loss)\n",
    "    \n",
    "    valid_loss = evaluate(model, valid_dl, loss_func)\n",
    "    print('Valid Loss: ', valid_loss)\n",
    "    \n",
    "    if valid_loss < best:\n",
    "        best = valid_loss\n",
    "        torch.save(model.state_dict(),  '../model/seq2seq_v2.pt')\n",
    "    scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/815 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "encoder output torch.Size([1, 319, 2048])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 319, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): Sizes of tensors must match except in dimension 1. Got 8 and 319 in dimension 0 (The offending index is 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-bb3a67db15e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Loss: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-183-bd307d21c942>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, lossFun, backwards, print_loss)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6157\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-179-4d60f39e9cea>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m#insert input token embedding, previous hidden state and the context state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m#receive output tensor (predictions) and new hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m#place predictions in a tensor holding predictions for each token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-178-cbf21ecbd434>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_input, encoded_context, hidden)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0memb_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): Sizes of tensors must match except in dimension 1. Got 8 and 319 in dimension 0 (The offending index is 1)"
     ]
    }
   ],
   "source": [
    "# train 3 more epochs\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('Epoch: ', epoch)\n",
    "    \n",
    "    train_loss = train_one_epoch(model, train_dl, optimizer, loss_func)\n",
    "    print('Train Loss: ', train_loss)\n",
    "    \n",
    "    valid_loss = evaluate(model, valid_dl, loss_func)\n",
    "    print('Valid Loss: ', valid_loss)\n",
    "    \n",
    "    if valid_loss < best:\n",
    "        best = valid_loss\n",
    "        torch.save(model.state_dict(),  '../model/seq2seq_v2.pt')\n",
    "    scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8.3415e-07,  1.8757e-08, -1.0000e+00, -7.6159e-01,  2.7992e-12,\n",
       "          -3.5686e-05,  8.0224e-15, -1.5713e-10, -1.4366e-13,  3.3050e-13,\n",
       "           2.0661e-21,  1.0000e+00, -1.2542e-10, -4.9077e-11, -9.7051e-05,\n",
       "           1.3208e-20,  7.0467e-07,  1.1476e-12, -1.8057e-11,  8.1142e-15,\n",
       "          -3.8975e-02, -1.4302e-16, -1.0386e-09, -2.6230e-12,  7.6159e-01,\n",
       "          -2.6330e-23, -1.2532e-02, -5.2528e-04, -1.2949e-07,  2.4892e-08,\n",
       "           6.0026e-07, -1.1196e-10,  1.5296e-10, -5.0617e-09,  2.4082e-09,\n",
       "          -3.3455e-08, -1.0477e-16,  4.9381e-16, -9.8111e-22,  1.8580e-14,\n",
       "          -4.9357e-11,  1.0770e-11,  4.9214e-10,  9.3780e-16, -6.4904e-04,\n",
       "           1.0489e-06,  5.5468e-14,  1.2570e-05, -5.2065e-02, -1.0022e-04,\n",
       "          -1.2523e-10, -3.3691e-08,  1.9675e-05, -1.4091e-15,  1.4231e-01,\n",
       "           1.4667e-12,  5.1739e-10, -5.8571e-08, -3.2723e-04,  1.1033e-10,\n",
       "          -3.1379e-28, -2.8651e-10, -1.4404e-19, -9.0455e-08, -7.8566e-11,\n",
       "           1.3765e-10,  4.9446e-08,  2.5476e-09, -3.1349e-08, -2.8717e-14,\n",
       "           4.1009e-12,  9.0636e-10,  1.3285e-11,  4.4793e-11, -5.4704e-17,\n",
       "          -1.9342e-10, -7.6961e-11, -5.8884e-17,  9.2462e-03, -1.0238e-11,\n",
       "          -2.8134e-13,  7.6159e-01,  2.5199e-09, -1.2039e-12, -1.7211e-16,\n",
       "          -1.4748e-07, -8.5063e-12,  4.6814e-16,  1.0759e-16, -1.0000e+00,\n",
       "          -2.6427e-26, -1.0770e-02,  3.1461e-09,  2.5775e-08,  3.7092e-26,\n",
       "           1.6739e-11,  2.0317e-25, -4.5702e-12,  8.0046e-14, -6.0396e-11,\n",
       "           4.7022e-25, -4.7724e-07, -7.5799e-11, -8.4636e-10,  2.0097e-13,\n",
       "           1.0125e-06,  1.7511e-10,  9.7946e-11,  1.1944e-14, -1.0000e+00,\n",
       "           1.0000e+00, -9.7485e-08,  9.7563e-09, -1.0528e-11, -1.0930e-03,\n",
       "           1.8838e-08,  2.3630e-07,  2.1257e-13, -2.5075e-15, -5.2711e-07,\n",
       "           1.3443e-29, -1.3560e-03,  1.3562e-06, -9.9305e-19,  1.0000e+00,\n",
       "           2.6614e-07,  4.3066e-09,  2.4873e-14, -1.4642e-09,  2.3889e-22,\n",
       "           4.3710e-14, -4.6720e-02,  5.6218e-09,  1.5116e-11,  5.6153e-09,\n",
       "           1.0000e+00, -4.6661e-09,  9.7760e-22,  8.9010e-14, -1.6127e-11,\n",
       "          -2.2074e-12,  4.4158e-27, -7.6159e-01, -3.8015e-02,  6.4763e-20,\n",
       "          -6.4404e-08,  9.1023e-09, -1.8285e-13,  9.1019e-12, -1.2197e-08,\n",
       "           2.7767e-10,  6.7147e-10, -9.1421e-09, -5.5355e-08,  3.4110e-11,\n",
       "           2.4689e-12, -2.0813e-07,  2.4052e-14,  2.0371e-09, -8.0117e-24,\n",
       "           1.5338e-21, -4.6145e-03,  1.0000e+00,  6.5855e-11,  6.0851e-11,\n",
       "          -1.2440e-07,  6.6149e-21, -5.2121e-10,  1.0000e+00,  1.6491e-08,\n",
       "          -5.8150e-11, -4.8320e-11,  1.2325e-08, -4.2827e-09,  1.1008e-13,\n",
       "          -1.0000e+00,  5.8404e-11, -3.8083e-10, -1.0000e+00,  6.7080e-11,\n",
       "           2.0297e-10,  1.0270e-01, -3.3564e-13,  1.8523e-15, -1.0000e+00,\n",
       "          -9.4786e-11, -7.0984e-06, -4.0065e-10,  5.5447e-08,  1.0000e+00,\n",
       "           3.0686e-11,  7.9642e-07,  7.0685e-12, -9.5438e-14, -8.2457e-05,\n",
       "          -7.6198e-07, -1.1899e-10, -3.6804e-17,  1.3207e-14, -7.7605e-14,\n",
       "          -9.9476e-01,  5.0469e-06,  4.0656e-16,  3.4020e-15,  1.9151e-06,\n",
       "          -4.0434e-26,  3.2205e-08, -4.3674e-09, -7.6144e-01, -7.3121e-14,\n",
       "          -1.3824e-07,  6.1466e-08, -7.3538e-26,  6.6500e-15, -3.3248e-07,\n",
       "           1.0000e+00,  3.6650e-13, -2.0898e-08,  4.9180e-12,  1.9031e-07,\n",
       "           5.0448e-12, -2.1450e-16, -7.6159e-01,  1.0000e+00,  3.7287e-17,\n",
       "           4.8356e-12,  1.0000e+00, -5.2864e-07, -4.7354e-11,  2.0782e-13,\n",
       "          -1.4318e-08, -9.3999e-11,  2.2857e-04, -1.3566e-10, -3.2166e-03,\n",
       "          -1.9981e-05,  1.4835e-03, -2.7394e-10,  9.2278e-12,  1.7502e-12,\n",
       "          -3.6867e-17, -1.1449e-10,  2.4320e-11, -6.1318e-17, -7.6149e-01,\n",
       "           5.2561e-09, -6.1599e-15, -1.0065e-01,  2.3076e-10,  3.1643e-15,\n",
       "          -5.5232e-01, -1.4257e-06, -2.2189e-13, -2.4821e-06, -5.9745e-14,\n",
       "           1.3974e-04, -5.1463e-22,  7.6159e-01, -2.4480e-03, -3.6327e-11,\n",
       "          -1.4107e-10,  4.0540e-10, -3.3417e-14, -1.3700e-10,  7.6159e-01,\n",
       "          -1.6639e-23,  5.8423e-27,  4.4754e-03, -1.0000e+00, -2.2025e-12,\n",
       "          -1.4419e-13, -7.6159e-01, -1.0093e-07, -4.1858e-17, -4.5342e-14,\n",
       "           1.7498e-10,  2.6384e-11, -4.4152e-09, -4.8146e-15,  2.5242e-06,\n",
       "          -1.9673e-06,  1.0000e+00, -3.9875e-08, -1.5969e-04,  1.5440e-09,\n",
       "          -2.7324e-10,  1.4477e-12, -1.8652e-16,  1.0548e-13, -7.5916e-11,\n",
       "           3.6071e-10,  5.2838e-22, -6.3665e-14, -3.3777e-13, -9.8407e-14,\n",
       "           7.6159e-01,  1.3633e-11,  2.8580e-10,  9.2228e-16, -1.4475e-05,\n",
       "          -1.0058e-09, -5.8477e-07, -6.5089e-09,  4.7209e-09,  2.1132e-16,\n",
       "          -5.9605e-09,  1.4449e-11,  4.6788e-09, -9.3408e-04, -4.8944e-09,\n",
       "          -3.9525e-09, -4.7983e-11, -2.3320e-27,  2.1443e-09,  1.1054e-05,\n",
       "          -1.2043e-11,  2.3734e-08,  3.0749e-12, -2.0329e-09, -4.6275e-02,\n",
       "           1.2237e-06,  1.8574e-10, -1.2889e-17,  7.3204e-15, -7.6159e-01,\n",
       "           1.0000e+00, -8.4847e-13, -7.6159e-01,  4.5644e-12, -4.2921e-11,\n",
       "           4.0110e-08, -6.0500e-07, -1.3565e-08,  2.9219e-09,  4.7153e-11,\n",
       "           1.0000e+00,  6.1012e-14, -9.0220e-01, -1.4923e-16,  6.8048e-10,\n",
       "           3.0781e-17, -1.0078e-13, -1.2629e-09,  2.7254e-12, -8.8841e-11,\n",
       "           9.5877e-17,  3.2370e-10,  1.1602e-10,  5.3071e-11,  2.5113e-10,\n",
       "          -1.0145e-09,  1.9289e-12, -2.2038e-18,  2.2842e-08,  3.9374e-09,\n",
       "          -2.9938e-08,  1.4297e-30, -1.6779e-18, -3.4992e-11,  1.2163e-09,\n",
       "           8.7093e-08,  2.6112e-12,  3.6172e-12, -1.0756e-09,  3.4260e-09,\n",
       "          -7.5577e-09, -5.0272e-10,  8.0846e-11,  4.5662e-14,  1.6713e-09,\n",
       "          -3.0490e-05,  1.3711e-13, -5.4495e-10,  3.5678e-10,  2.5634e-11,\n",
       "           4.7728e-11,  1.5807e-08,  3.8139e-10,  2.0863e-11,  1.1218e-09,\n",
       "           1.1221e-01,  6.5520e-26,  7.6177e-10,  4.2122e-11, -1.5374e-03,\n",
       "          -2.8512e-11,  3.7352e-02,  1.0000e+00, -1.0000e+00, -1.0765e-06,\n",
       "           1.0555e-15,  3.1016e-11, -1.2957e-09, -1.1898e-07,  1.0932e-10,\n",
       "          -9.6253e-09, -2.0374e-24,  8.0792e-07,  1.1558e-09,  9.9283e-07,\n",
       "          -1.1455e-13,  7.5371e-09, -2.3519e-09, -1.3080e-10,  1.6785e-06,\n",
       "          -7.4822e-13,  7.0594e-15,  1.0701e-12,  6.1955e-15,  3.2217e-06,\n",
       "          -2.2987e-06, -3.0029e-20,  1.3897e-10,  9.9999e-01, -6.0170e-16,\n",
       "           4.1747e-04,  1.4579e-12, -6.2663e-16,  4.6652e-11,  6.8469e-10,\n",
       "           1.0785e-07, -3.0025e-15,  1.0000e+00,  1.1956e-20, -1.0000e+00,\n",
       "           6.1595e-07, -9.6382e-05, -3.9603e-11,  7.6159e-01, -2.7042e-28,\n",
       "           9.4996e-13, -3.4837e-09, -1.4055e-06, -2.9560e-06, -2.3441e-08,\n",
       "           1.7434e-01,  3.1450e-07,  6.9858e-10,  2.2650e-06,  7.5632e-01,\n",
       "          -8.5240e-25,  2.4636e-10, -1.6547e-16, -4.5360e-11,  1.3631e-18,\n",
       "           1.7080e-14,  5.2297e-08,  2.6665e-11, -2.1053e-05,  6.0264e-14,\n",
       "          -5.2385e-15, -1.1067e-07, -1.5459e-12, -9.6062e-12,  1.6440e-10,\n",
       "          -1.9503e-13,  1.7698e-09,  7.6160e-01,  1.4645e-05, -1.9994e-12,\n",
       "          -5.4941e-20, -1.9083e-11, -3.0622e-08, -1.2029e-07,  1.4261e-04,\n",
       "           3.0314e-09,  4.9384e-08, -1.3734e-05,  1.0000e+00,  1.4927e-08,\n",
       "          -5.2514e-17,  2.5863e-10, -1.0000e+00,  2.6630e-09,  6.2905e-03,\n",
       "           3.6822e-15,  2.3792e-13,  9.6616e-20,  7.6159e-01, -4.2835e-10,\n",
       "           4.8274e-13, -1.8301e-15,  2.7109e-12, -4.2892e-24, -2.3211e-08,\n",
       "           3.8126e-09, -1.3649e-06,  3.7600e-12, -4.2794e-13, -3.1722e-25,\n",
       "          -5.5280e-20,  4.8895e-08,  1.4968e-09, -5.0621e-09, -7.8561e-14,\n",
       "          -3.6016e-15, -2.9086e-12, -1.0316e-03,  4.0109e-10, -2.1743e-02,\n",
       "          -1.6683e-01,  3.1676e-33, -3.1692e-19,  7.8820e-06, -9.3719e-13,\n",
       "          -1.8513e-09,  1.1684e-16, -3.9326e-09,  1.3238e-07, -1.0000e+00,\n",
       "           3.3547e-11, -4.6864e-14,  1.0101e-20, -6.4408e-20,  1.1804e-07,\n",
       "           5.1590e-07,  1.2512e-09, -2.5929e-09, -3.6057e-05,  4.7752e-15,\n",
       "          -8.1391e-07, -2.8379e-08,  5.3851e-11, -1.0000e+00, -4.3296e-08,\n",
       "          -1.3523e-11, -1.9586e-07, -4.6160e-10, -4.8485e-15, -1.5846e-08,\n",
       "          -2.9453e-24, -7.8629e-10, -5.8177e-06, -1.5854e-11,  4.1974e-07,\n",
       "           2.8464e-12,  4.6945e-08,  9.1518e-09, -1.4985e-16,  7.0716e-02,\n",
       "           6.3613e-08,  1.0000e+00, -3.9775e-10,  7.6159e-01,  1.3115e-35,\n",
       "          -1.3815e-09,  2.8125e-08,  1.4548e-02, -1.7705e-12,  1.2035e-15,\n",
       "          -1.6492e-02, -5.3461e-09,  5.6090e-16, -4.9492e-09, -2.1196e-04,\n",
       "           1.3934e-11,  3.9153e-07, -1.8111e-11, -9.8962e-05,  8.9453e-06,\n",
       "           2.9542e-14,  3.3710e-08,  9.0423e-10,  2.4779e-27, -4.8405e-21,\n",
       "           1.0000e+00, -7.2679e-07, -1.0000e+00,  1.6877e-11,  1.2025e-07,\n",
       "           1.8433e-18, -6.0712e-11, -7.4386e-11,  2.2427e-09, -4.6506e-10,\n",
       "           1.2461e-09,  2.6077e-05,  1.0074e-11,  5.0857e-11, -2.6322e-09,\n",
       "           2.5371e-15,  2.9549e-11,  1.9869e-15,  2.4526e-12, -1.0000e+00,\n",
       "          -1.5903e-11, -3.0295e-04, -4.4557e-16,  2.0546e-11,  4.8306e-22,\n",
       "           2.9260e-11, -9.3240e-21, -2.0892e-10,  3.1496e-09,  1.9104e-16,\n",
       "          -9.0668e-12, -2.1620e-10,  4.5663e-15,  1.0000e+00, -1.8464e-12,\n",
       "           1.2578e-07, -6.4666e-13,  1.5219e-10,  3.8444e-08,  1.4972e-17,\n",
       "          -1.9575e-10, -1.8429e-13,  1.2215e-14,  1.5118e-08, -6.5224e-08,\n",
       "           3.5603e-08, -2.4907e-10, -6.6299e-08, -4.0116e-09,  1.0257e-11,\n",
       "           2.7645e-08, -7.6159e-01, -3.1893e-11,  4.6584e-06, -1.0786e-11,\n",
       "          -1.0752e-10,  1.4273e-11, -1.2721e-09, -5.5312e-10,  1.1148e-02,\n",
       "          -9.7147e-15,  8.5599e-11,  4.4218e-09, -9.5257e-31, -9.8891e-09,\n",
       "          -1.0011e-05,  2.8926e-08,  6.4256e-16,  1.9018e-13, -1.9039e-22,\n",
       "           1.5072e-05,  1.0647e-08,  1.2581e-12, -1.2566e-09,  7.3255e-07,\n",
       "          -1.7681e-10, -9.9940e-10,  6.7118e-18,  6.5256e-09, -2.4323e-15,\n",
       "           1.3004e-08,  8.3823e-10, -7.9870e-10, -4.4808e-14,  2.8769e-11,\n",
       "           1.4666e-12, -4.6944e-10,  1.3147e-08,  5.2438e-20,  4.9397e-11,\n",
       "          -1.0000e+00, -3.2699e-09,  6.1717e-11, -1.6918e-09, -2.7133e-12,\n",
       "           3.1078e-10, -1.4057e-10, -1.0000e+00, -3.0822e-06,  4.9731e-08,\n",
       "           1.2374e-12,  2.1570e-08,  2.8260e-18, -3.4944e-08, -8.2411e-04,\n",
       "          -4.0212e-12, -3.5951e-02,  9.6121e-18, -2.7090e-06,  1.0838e-08,\n",
       "           8.4866e-09, -3.2158e-03,  1.3993e-03,  7.0009e-21, -7.1436e-12,\n",
       "           6.2904e-09, -7.6159e-01,  1.5544e-12, -8.1295e-09,  2.5074e-12,\n",
       "          -2.9300e-04, -2.6335e-29,  1.5366e-12, -3.7797e-10,  2.3093e-11,\n",
       "          -1.2398e-12,  8.9382e-11,  5.4210e-12,  3.8848e-09, -6.7352e-13,\n",
       "          -7.7398e-11,  2.8760e-35,  2.0964e-21,  1.5266e-14, -8.7174e-08,\n",
       "          -4.7285e-10,  2.6316e-14,  1.0349e-06, -4.9397e-04, -2.0679e-02,\n",
       "          -3.7752e-17, -4.0779e-08, -6.3816e-13,  2.4583e-09,  3.1125e-18,\n",
       "           1.0707e-07,  2.2753e-10, -8.8847e-07,  5.4379e-13, -5.1292e-03,\n",
       "          -4.0126e-15,  1.7044e-09,  8.9347e-09, -1.3989e-09,  2.7211e-10,\n",
       "           4.1499e-03, -8.2397e-01, -3.0989e-02, -4.2240e-11, -6.9282e-07,\n",
       "           1.9060e-14,  2.1969e-12,  3.5942e-09,  1.0000e+00, -1.8007e-08,\n",
       "           1.3574e-14,  1.7919e-10, -1.2696e-16,  5.6128e-11,  1.7713e-07,\n",
       "           2.0617e-14,  1.8149e-02, -1.6247e-07, -2.7812e-10, -1.0601e-09,\n",
       "           1.3909e-09,  2.1682e-08,  2.7390e-07, -1.1030e-14,  5.7847e-08,\n",
       "           2.4637e-03,  7.0279e-10, -4.0922e-12,  1.5467e-11,  3.6512e-07,\n",
       "           6.0392e-09, -1.3410e-10, -1.8193e-10, -1.8911e-06,  1.3519e-10,\n",
       "           1.0196e-21,  1.2184e-11,  6.2724e-12,  1.2503e-14,  1.8700e-07,\n",
       "          -6.2604e-12,  1.0000e+00,  4.4132e-09, -6.5667e-10, -1.8251e-09,\n",
       "           3.2559e-17,  1.3010e-09,  6.8135e-20,  2.6176e-10,  7.6159e-01,\n",
       "           6.7349e-25,  7.6159e-01, -5.1687e-10, -2.1046e-11,  2.1768e-18,\n",
       "          -7.4948e-02,  1.6387e-09, -2.5176e-09, -3.3847e-13, -4.1844e-13,\n",
       "          -9.3688e-10, -3.6494e-10,  1.0000e+00, -6.6247e-07, -2.4310e-07,\n",
       "           5.6255e-25,  4.0146e-13, -2.8625e-19,  1.5358e-07, -1.1666e-07,\n",
       "           1.2818e-08,  3.4116e-09, -9.8157e-12, -9.8172e-10, -6.0956e-19,\n",
       "          -3.7073e-24,  6.1957e-12,  1.0106e-16,  2.8816e-11,  2.2423e-24]]])"
      ]
     },
     "execution_count": 1058,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    context = model.encoder(test_x.unsqueeze(0))\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_seq(model, x, start_token, seq_length):\n",
    "    with torch.no_grad():\n",
    "        word_input = torch.tensor(word2idx[start_token]).unsqueeze(0)\n",
    "        context = model.encoder(x.unsqueeze(0))\n",
    "        hidden = context\n",
    "        outputs = [start_token]\n",
    "        ## generate a sequence!\n",
    "        for i in range(seq_length):\n",
    "            output, hidden = model.decoder(word_input, context, hidden)\n",
    "            word = output.argmax(1)\n",
    "            outputs.append(idx2word[word.item()])\n",
    "            word_input = word\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 2048])"
      ]
     },
     "execution_count": 1076,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x, test_y = next(iterator)\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,    4, 1082,   25, 5597,  104,   25,  219,    5, 2308,  305,   12,\n",
       "           4,  389,    2])"
      ]
     },
     "execution_count": 1081,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START>',\n",
       " 'a',\n",
       " 'teenage',\n",
       " 'girl',\n",
       " 'teases',\n",
       " 'another',\n",
       " 'girl',\n",
       " 'who',\n",
       " 'is',\n",
       " 'blindfolded',\n",
       " 'next',\n",
       " 'to',\n",
       " 'a',\n",
       " 'river',\n",
       " '<END>']"
      ]
     },
     "execution_count": 1082,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx2word[word.item()] for word in test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4117e-07,  8.7523e-09, -1.0000e+00, -7.6159e-01,  3.1522e-14,\n",
      "         -3.8256e-06,  1.7946e-15, -1.5878e-11, -6.9432e-16,  2.3584e-15,\n",
      "          1.6964e-22,  1.0000e+00, -2.9911e-12, -1.3260e-12, -6.8926e-03,\n",
      "          1.2900e-22,  3.9233e-08,  1.9082e-13, -6.0221e-12,  3.7145e-15,\n",
      "         -6.2369e-03, -7.8452e-15, -9.1389e-10, -1.4576e-13,  7.6159e-01,\n",
      "         -3.6218e-25, -4.9680e-03, -2.1962e-04, -1.4119e-09, -1.8743e-08,\n",
      "          6.3280e-08, -1.4303e-11,  1.7118e-11, -1.7941e-09,  1.9121e-10,\n",
      "         -2.3796e-08, -3.5009e-14,  7.3931e-16, -4.8089e-22,  2.3265e-13,\n",
      "         -8.8118e-11,  5.9196e-12,  6.9587e-11,  5.1894e-16, -4.3336e-06,\n",
      "          1.3440e-05,  2.3725e-14,  3.2155e-04, -4.9125e-01, -4.2814e-04,\n",
      "         -1.0928e-10, -4.0647e-09,  1.0124e-05, -4.2381e-17,  6.6224e-02,\n",
      "          1.7039e-13,  4.5293e-11, -2.1232e-09, -5.8124e-06,  2.0353e-11,\n",
      "         -5.4074e-28, -4.1671e-11,  9.0823e-21, -4.6344e-07, -4.0680e-09,\n",
      "          1.1559e-11,  4.7354e-08,  2.8662e-10, -3.7436e-08, -3.7561e-15,\n",
      "          6.2532e-13,  2.0632e-09,  3.9544e-10,  2.5569e-13, -6.9883e-17,\n",
      "         -3.0408e-11, -3.3246e-12, -2.9005e-17,  6.2029e-02, -1.8190e-10,\n",
      "         -3.6354e-14,  7.6149e-01,  4.2224e-10,  1.5928e-14, -6.9256e-17,\n",
      "         -5.4321e-08, -9.6946e-13,  9.4571e-16,  7.4187e-17, -1.0000e+00,\n",
      "         -1.1590e-25, -1.0380e-02,  3.0847e-10,  6.9025e-10,  4.6102e-27,\n",
      "          1.4606e-12,  1.1103e-27, -7.4015e-14,  1.4983e-15, -2.6031e-14,\n",
      "          1.9532e-27, -2.1862e-06, -5.3637e-15, -1.9830e-09,  5.0210e-14,\n",
      "          6.4224e-08,  2.2588e-10,  1.4480e-11,  5.1450e-15, -1.0000e+00,\n",
      "          1.0000e+00, -5.7192e-08,  6.2660e-10, -1.1870e-11, -1.6119e-10,\n",
      "          1.1713e-08,  8.9438e-09,  1.1683e-14, -3.0721e-17, -6.7020e-07,\n",
      "          5.0476e-28, -4.4026e-04,  1.6557e-06, -1.4472e-17,  1.0000e+00,\n",
      "          9.5120e-08,  2.8691e-11,  6.0137e-16, -1.5287e-08,  3.0611e-24,\n",
      "          1.5935e-12, -1.1986e-02,  7.2924e-10,  8.0699e-12,  1.8389e-08,\n",
      "          1.0000e+00, -2.0764e-07,  1.5540e-22,  2.7722e-14, -4.9706e-12,\n",
      "         -7.2602e-11,  6.5146e-28, -7.6159e-01, -1.0557e-01,  1.6052e-18,\n",
      "         -2.5803e-07,  1.0888e-09, -5.7935e-13,  2.1470e-10, -4.5960e-09,\n",
      "          1.8827e-09,  6.4487e-12, -9.8835e-11, -2.5798e-09,  2.0480e-12,\n",
      "          3.6949e-12, -1.5605e-07,  5.4132e-16,  1.9886e-10, -3.9069e-23,\n",
      "          2.5015e-20, -3.0370e-02,  1.0000e+00,  7.1888e-11,  3.4571e-11,\n",
      "         -5.4059e-08,  1.6359e-20, -4.5683e-09,  9.9999e-01,  1.7351e-10,\n",
      "         -1.1338e-10, -3.9823e-11,  2.6220e-09, -1.3732e-10,  8.7649e-15,\n",
      "         -1.0000e+00,  4.7112e-11,  4.6365e-09, -1.0000e+00,  2.4144e-10,\n",
      "          7.5875e-12,  1.5687e-02, -7.6119e-14,  2.4740e-16, -1.0000e+00,\n",
      "         -5.7678e-10, -7.3192e-06, -6.0900e-12,  2.9233e-08,  1.0000e+00,\n",
      "          6.9372e-11,  9.0171e-07,  2.1617e-12, -1.7255e-15, -8.6092e-06,\n",
      "         -4.0599e-05, -1.8790e-10, -9.7500e-17,  9.0255e-15, -9.6399e-14,\n",
      "         -9.9726e-01,  2.4485e-07,  2.1082e-17,  5.7377e-15,  6.6742e-07,\n",
      "         -5.7819e-26,  2.1302e-09, -2.9053e-10, -7.6146e-01, -5.2360e-15,\n",
      "         -2.0819e-08,  3.4933e-07, -6.2427e-25,  1.3368e-16, -2.5033e-07,\n",
      "          1.0000e+00,  1.5628e-14, -8.2989e-08,  7.4026e-11,  1.5359e-07,\n",
      "          9.5360e-11, -1.4193e-14, -7.6159e-01,  1.0000e+00,  9.1913e-17,\n",
      "          2.5102e-12,  1.0000e+00, -3.0106e-06, -7.6827e-13,  1.7749e-12,\n",
      "         -1.7295e-09, -2.5308e-11,  2.2078e-04, -3.3322e-12, -7.4892e-05,\n",
      "         -1.8172e-10,  7.7724e-03, -5.9775e-09,  3.1077e-16,  2.3391e-13,\n",
      "         -1.6829e-15, -1.4821e-11,  2.8355e-12, -2.3407e-17, -7.6149e-01,\n",
      "          1.8617e-09, -3.0505e-16, -3.8228e-04,  1.0844e-10,  5.4246e-14,\n",
      "         -2.6865e-05,  6.3427e-09, -3.0115e-15, -1.7472e-08, -3.5526e-15,\n",
      "          1.0696e-03, -1.9761e-22,  7.6159e-01, -8.4805e-03, -2.6167e-11,\n",
      "         -7.2835e-11,  8.1107e-10, -1.2330e-16, -4.1332e-11,  7.6159e-01,\n",
      "          2.3867e-21,  8.6312e-26,  1.3705e-02, -1.0000e+00, -5.6145e-14,\n",
      "         -1.1421e-13, -7.6159e-01, -3.0273e-08, -1.1595e-17, -5.9498e-14,\n",
      "          6.9987e-11,  1.2391e-12, -3.5926e-09, -2.5420e-17,  8.8128e-07,\n",
      "         -4.7335e-05,  1.0000e+00, -4.4238e-09, -4.1015e-05,  2.8415e-10,\n",
      "         -1.0095e-11,  3.5823e-13, -3.3867e-16,  1.0297e-15, -5.1245e-12,\n",
      "          1.8959e-09,  1.0820e-27, -1.5409e-14, -7.7473e-14, -4.6230e-14,\n",
      "          7.6159e-01,  4.7449e-10,  1.0959e-09,  3.3426e-16, -2.2245e-02,\n",
      "         -1.7266e-08, -1.1263e-06, -6.9595e-09,  7.8378e-10,  1.0415e-17,\n",
      "         -1.0256e-09,  2.8934e-13,  3.3835e-09, -3.1443e-03, -2.5775e-10,\n",
      "         -1.9229e-08, -2.1684e-10, -2.7806e-29,  2.0334e-10,  8.1621e-09,\n",
      "         -2.3713e-12,  1.3256e-08,  1.4616e-12, -5.5373e-09, -1.0774e-01,\n",
      "          2.1373e-06,  3.7278e-11, -7.7566e-18,  1.9618e-13, -7.6159e-01,\n",
      "          1.0000e+00, -2.4397e-13, -7.6159e-01,  7.4537e-14, -2.6240e-11,\n",
      "          1.5787e-08, -7.3339e-07, -1.1540e-08,  1.9391e-08,  7.7935e-10,\n",
      "          1.0000e+00,  4.2305e-13, -9.9257e-01, -3.5747e-16,  2.3759e-10,\n",
      "          1.4193e-18, -1.2610e-13, -7.4174e-09,  4.3817e-13, -1.1871e-10,\n",
      "          8.6491e-16,  4.4588e-10, -2.0193e-10,  5.8526e-13,  7.2654e-11,\n",
      "         -7.2221e-11,  2.4813e-16,  9.2645e-18,  3.3722e-08,  3.9729e-11,\n",
      "         -4.1256e-08,  1.6494e-30, -3.2448e-19, -7.3115e-13,  5.3978e-10,\n",
      "          2.6623e-09,  2.6309e-12,  1.1344e-13, -4.0422e-08,  1.9503e-09,\n",
      "         -3.5239e-13, -5.8951e-09,  7.4276e-10,  1.5624e-12,  5.2836e-11,\n",
      "         -1.4405e-05,  1.1782e-13, -7.4189e-10,  6.1547e-10,  6.2220e-11,\n",
      "          2.0195e-11,  4.9730e-08,  4.2716e-10,  3.0944e-10,  1.1339e-10,\n",
      "          2.3477e-03,  3.9415e-26,  7.5653e-11,  1.5130e-11, -5.5048e-04,\n",
      "         -4.7897e-11,  4.0087e-02,  1.0000e+00, -1.0000e+00, -1.5208e-07,\n",
      "          1.6419e-16,  2.6392e-10, -2.3713e-09, -2.3509e-08,  4.8261e-12,\n",
      "         -3.7136e-09, -5.2525e-25,  6.4306e-06,  1.1439e-10,  3.3300e-06,\n",
      "         -7.6633e-13,  8.8600e-11, -1.8566e-09, -4.2415e-10,  2.5614e-06,\n",
      "         -3.1022e-12,  1.9882e-15,  1.1271e-13,  5.6278e-15,  4.7332e-05,\n",
      "         -1.4750e-06, -2.6308e-32,  1.2399e-10,  1.0000e+00, -3.6163e-17,\n",
      "          3.6886e-03,  1.6916e-13, -3.8392e-16,  1.1310e-10,  3.2605e-08,\n",
      "          8.8740e-09, -1.8178e-14,  1.0000e+00,  3.5008e-20, -1.0000e+00,\n",
      "          3.1274e-08, -7.3540e-05, -5.1719e-12,  7.6159e-01, -3.4652e-28,\n",
      "          1.6255e-13, -6.3225e-09, -6.2576e-07, -1.3482e-05, -6.2789e-09,\n",
      "          2.8319e-02,  8.0097e-07,  2.1955e-10,  1.5472e-05,  6.0343e-01,\n",
      "         -1.9646e-27,  5.5687e-10, -1.6656e-17, -2.1939e-10,  1.6629e-19,\n",
      "          1.1384e-12,  7.1345e-08,  1.8387e-11, -6.7678e-04,  1.7721e-15,\n",
      "         -8.6402e-16, -1.1080e-07, -1.0582e-12, -3.9951e-14,  7.2695e-12,\n",
      "         -8.5448e-14,  1.7687e-10,  7.6159e-01,  3.1165e-06, -3.3038e-11,\n",
      "         -4.5536e-22, -2.8361e-10, -4.6749e-09, -8.7432e-09,  5.8803e-06,\n",
      "          4.6824e-10,  8.3651e-07, -3.1462e-05,  1.0000e+00,  3.3722e-08,\n",
      "         -2.9231e-22,  1.1097e-09, -1.0000e+00,  2.8563e-08,  1.3241e-02,\n",
      "          7.3233e-17,  3.1056e-13,  3.5401e-19,  7.6159e-01, -2.4339e-10,\n",
      "          1.2650e-14, -4.1841e-15,  1.0073e-13, -6.4758e-24, -1.1075e-09,\n",
      "          9.2587e-10, -6.0008e-07,  4.0688e-13, -1.8207e-13, -4.8923e-26,\n",
      "         -1.7420e-21,  8.7554e-09,  1.1376e-08, -4.4900e-10, -9.1370e-15,\n",
      "         -1.7988e-16, -2.6569e-12, -6.8553e-03,  1.1413e-10, -7.9636e-04,\n",
      "         -6.5949e-02,  1.5679e-32, -1.2419e-19,  1.0925e-04, -1.7770e-13,\n",
      "         -1.7137e-12,  9.4637e-18, -1.0125e-09,  1.7198e-07, -1.0000e+00,\n",
      "          3.2412e-11, -4.1830e-15,  3.8969e-23, -1.5182e-21,  1.9724e-09,\n",
      "          9.3991e-08,  1.4568e-09, -3.1820e-10, -3.3634e-08,  1.3525e-14,\n",
      "         -3.4539e-07, -2.4906e-09,  4.0240e-10, -1.0000e+00, -5.5455e-07,\n",
      "         -3.5129e-12, -3.0837e-07, -3.0624e-11, -9.3472e-16, -3.6397e-08,\n",
      "         -2.0868e-26, -2.1006e-09, -1.2685e-06, -5.2853e-11,  5.8995e-07,\n",
      "          5.9547e-11,  6.1866e-07,  2.3210e-09, -4.9252e-17,  4.9831e-02,\n",
      "          1.5727e-08,  1.0000e+00, -2.7648e-11,  7.6159e-01,  1.5059e-34,\n",
      "         -2.7834e-10,  1.4460e-10,  3.6196e-02, -4.4698e-13,  3.9453e-15,\n",
      "         -8.5393e-02, -1.6842e-07,  1.1232e-15, -1.4696e-09, -4.7414e-04,\n",
      "          1.5349e-10,  4.8244e-08, -3.4383e-12, -1.4993e-03,  7.9052e-06,\n",
      "          3.7914e-13,  9.1726e-09,  9.4922e-10,  4.6765e-27, -4.0832e-30,\n",
      "          1.0000e+00, -1.7869e-07, -1.0000e+00,  1.3999e-12,  3.1964e-08,\n",
      "          8.8748e-20, -6.3452e-12, -1.0730e-12,  1.6067e-09, -9.1809e-11,\n",
      "          6.9910e-09,  3.5725e-06,  8.0569e-11,  1.3688e-09, -1.8264e-09,\n",
      "          1.4886e-17,  3.0218e-12,  1.9658e-16,  1.9727e-12, -1.0000e+00,\n",
      "         -3.3710e-12, -7.6421e-03, -4.2873e-19,  1.6984e-11,  5.5680e-22,\n",
      "          3.4380e-11, -1.9649e-19,  3.7844e-10,  3.0502e-09,  2.8270e-17,\n",
      "         -9.4595e-12, -3.4353e-10,  1.4555e-16,  1.0000e+00, -7.8783e-13,\n",
      "          1.3530e-07, -1.3736e-13,  7.5945e-11,  3.7615e-08,  1.8348e-17,\n",
      "         -1.2461e-12, -1.0916e-13,  1.4398e-16,  8.9054e-10, -5.9840e-11,\n",
      "          6.2739e-08, -2.0034e-13, -4.9394e-08, -1.2011e-10,  5.6463e-15,\n",
      "          6.9865e-09, -7.6159e-01, -5.1026e-13,  1.0399e-06, -1.4847e-10,\n",
      "         -5.3218e-11,  7.5937e-12, -1.4409e-10, -2.4531e-09,  2.3266e-02,\n",
      "         -1.3769e-14,  5.3369e-12,  2.1179e-09, -1.8869e-32, -1.4122e-10,\n",
      "         -5.8087e-08,  3.1744e-08, -5.8721e-18,  1.9446e-12, -1.2793e-22,\n",
      "          1.4774e-05,  9.7416e-09,  9.9138e-13, -9.0729e-10,  1.9945e-07,\n",
      "         -1.0438e-09, -4.4558e-11,  6.3284e-19,  2.1622e-09, -3.3039e-17,\n",
      "          1.6191e-09,  5.3645e-08,  4.4746e-09, -8.4829e-15,  2.7593e-10,\n",
      "          6.1674e-12, -2.8141e-09,  7.9385e-10,  1.3497e-19,  2.9746e-12,\n",
      "         -1.0000e+00, -1.8682e-09,  1.6333e-11, -9.3504e-11, -9.9482e-13,\n",
      "          3.5098e-09, -3.9344e-10, -1.0000e+00, -2.9784e-05,  2.2254e-07,\n",
      "          7.4935e-13,  2.5343e-09,  3.8036e-19, -7.5648e-08, -2.6596e-03,\n",
      "         -1.0301e-12, -1.0676e-01,  1.0790e-17, -2.3080e-05,  3.0900e-09,\n",
      "          2.0995e-07, -1.6127e-02,  1.9554e-03,  1.2966e-22, -2.2634e-11,\n",
      "          1.0531e-08, -7.6159e-01,  9.2592e-13, -9.7153e-10,  8.0327e-13,\n",
      "         -3.7925e-05, -1.1155e-28,  3.0986e-13, -6.9145e-10,  2.7848e-12,\n",
      "         -2.2563e-11,  1.3677e-12,  3.6969e-13,  3.1914e-09, -4.8573e-14,\n",
      "         -1.6780e-12,  2.6248e-36,  1.0008e-19,  2.0923e-14, -2.2811e-08,\n",
      "         -3.0926e-10,  3.9965e-14,  2.6131e-09, -6.4564e-04, -4.6896e-03,\n",
      "         -2.1446e-20, -1.1155e-09, -6.7710e-15,  1.8580e-08,  5.8405e-18,\n",
      "          2.2210e-07,  7.1139e-10, -1.2789e-06,  2.4396e-13, -7.5539e-03,\n",
      "         -1.3501e-14,  8.1098e-11,  3.4330e-08, -4.5698e-10,  7.3837e-12,\n",
      "          7.3671e-03, -9.7660e-01, -1.2524e-01, -6.5852e-13, -9.7861e-07,\n",
      "          1.5530e-12,  5.9318e-14,  1.0042e-07,  1.0000e+00, -5.4036e-08,\n",
      "          8.1218e-14,  1.0062e-10, -2.2571e-17,  1.2942e-11,  4.6399e-09,\n",
      "          1.6428e-15,  3.0889e-03, -1.0591e-06, -3.3350e-11, -1.1716e-08,\n",
      "          4.7801e-11,  1.2833e-10,  3.8974e-06, -2.0278e-15,  3.9878e-08,\n",
      "          2.1680e-02,  4.2529e-11, -2.8248e-12,  2.7899e-11,  3.7562e-07,\n",
      "          1.8168e-09, -8.8185e-11, -5.8098e-11, -1.6056e-07,  1.2470e-10,\n",
      "          1.0103e-22,  6.6172e-13,  2.1675e-12,  3.6789e-19,  1.0621e-13,\n",
      "         -1.4468e-10,  1.0000e+00,  1.2587e-08, -2.0389e-09, -2.9217e-10,\n",
      "          5.4311e-18,  1.0781e-08,  1.6350e-22,  1.4750e-09,  7.6159e-01,\n",
      "          1.8868e-24,  7.6159e-01, -2.0763e-09, -3.8427e-11,  7.6024e-19,\n",
      "         -1.4530e-01,  3.1491e-10, -9.9076e-11, -1.0852e-12, -4.4555e-14,\n",
      "         -1.6537e-08, -1.7832e-11,  1.0000e+00, -4.2561e-06, -1.7188e-07,\n",
      "          2.9461e-24,  6.5546e-14, -2.9698e-21,  7.6590e-07, -1.7653e-09,\n",
      "          1.4058e-08,  1.8002e-10,  2.0840e-12, -1.1256e-09,  1.1561e-19,\n",
      "         -4.0458e-23,  1.8863e-10,  1.4637e-17,  1.9066e-10,  3.2445e-28]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<START>',\n",
       " 'a',\n",
       " 'man',\n",
       " 'is',\n",
       " 'talking',\n",
       " 'about',\n",
       " 'a',\n",
       " 'car',\n",
       " '<END>',\n",
       " '<END>',\n",
       " 'his',\n",
       " 'father',\n",
       " 'in',\n",
       " 'a',\n",
       " 'home',\n",
       " '<END>']"
      ]
     },
     "execution_count": 1078,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_seq(model, test_x,  '<START>', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
