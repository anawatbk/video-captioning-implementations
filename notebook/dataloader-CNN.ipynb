{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path configuration\n",
    "LABELS_PATH = '../data/train_val_annotation/train_val_videodatainfo.json'\n",
    "DATA_PATH = '../data/train_val_features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON file\n",
    "f = open (LABELS_PATH, \"r\")\n",
    "  # Reading from file\n",
    "data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 9,\n",
       " 'url': 'https://www.youtube.com/watch?v=9lZi22qLlEo',\n",
       " 'video_id': 'video0',\n",
       " 'start time': 137.72,\n",
       " 'end time': 149.44,\n",
       " 'split': 'train',\n",
       " 'id': 0}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['videos'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## randomly select sentence\n",
    "#label_df = pd.DataFrame(data['sentences'])\n",
    "#label_final_df = label_df.groupby('video_id')['sen_id'].unique().apply(lambda x: x[np.random.randint(0,20)]).to_frame().reset_index()\n",
    "#label_final_df['video_id'].nunique()\n",
    "#label_final_df = label_final_df.join(label_df[['sen_id', 'caption']].set_index('sen_id'), on='sen_id')\n",
    "#label_final_df.to_csv('../data/label_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_final_df = pd.read_csv('../data/label_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create embedding matrix from google news word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load('en_core_web_sm', disable = ['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "# embedding\n",
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "glove_emb = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sent = label_final_df['caption'].tolist()#.astype('unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = Counter()\n",
    "try:\n",
    "    for doc in nlp.pipe(all_sent):\n",
    "        for word in doc:\n",
    "            #print(word)\n",
    "            wc[str(word)] += 1\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(doc,'\\nword:', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding matrix\n",
    "# initialization\n",
    "EMBEDDING_SIZE = 300\n",
    "embedding = np.zeros((len(wc)+4, 300)) # +4 for start, end, unk, padding\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "\n",
    "word2idx['<PAD>'] = 0\n",
    "idx2word[0] = '<PAD>'\n",
    "embedding[0] = np.random.rand(300)*2 - 1\n",
    "\n",
    "word2idx['<START>'] = 1\n",
    "idx2word[1] = '<START>'\n",
    "embedding[1] = np.random.rand(300)*2 - 1\n",
    "\n",
    "word2idx['<END>'] = 2\n",
    "idx2word[2] = '<END>'\n",
    "embedding[2] = np.random.rand(300)*2 - 1\n",
    "\n",
    "word2idx['<UNK>'] = 3\n",
    "idx2word[3] = '<UNK>'\n",
    "embedding[3] = np.random.rand(300)*2 - 1\n",
    "\n",
    "count = 0\n",
    "for word, _ in wc.most_common():\n",
    "    wid = len(word2idx)\n",
    "    word2idx[word] = wid\n",
    "    idx2word[wid] = word\n",
    "    if word in glove_emb:\n",
    "        embedding[wid] = glove_emb.get_vector(word)\n",
    "    else:\n",
    "        embedding[wid] = np.random.rand(300)*2 - 1 # random initialisation (-1, 1)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 words are not in google news werd2vec\n"
     ]
    }
   ],
   "source": [
    "print(f'{count} words are not in google news werd2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../model/embedding.npy', embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../model/word2idx.pkl\",\"wb\") as f:\n",
    "    pickle.dump(word2idx, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../model/idx2word.pkl\",\"wb\") as f:\n",
    "    pickle.dump(idx2word, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Dataset Class for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSRVTT(Dataset):\n",
    "    def __init__(self, df, word2idx, DATA_PATH):\n",
    "        super(MSRVTT, self).__init__()\n",
    "        self.df = df\n",
    "        self.path = DATA_PATH\n",
    "        self.word2idx = word2idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        vid = row['video_id']\n",
    "        filename = self.path + f'{vid}-feature.pt5'\n",
    "        x = torch.load(filename)\n",
    "        sentence_emb = [self.word2idx.get(word, self.word2idx['<UNK>']) for word in row['caption'].split(' ')]\n",
    "        y = torch.zeros(len(sentence_emb)+2)\n",
    "        y[0], y[-1] = self.word2idx['<START>'], self.word2idx['<END>']\n",
    "        y[1:-1] = torch.tensor(sentence_emb)\n",
    "        #true_sentence = row['caption']\n",
    "        y = F.pad(y, pad=(0, 20 - y.shape[0]))\n",
    "        x = F.pad(x, pad=(0, 0, 40-x.shape[0], 0))\n",
    "        return x, y.long()#, true_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MSRVTT(label_final_df, word2idx, DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0396, 0.4085, 0.5028,  ..., 0.6331, 0.1015, 0.0470],\n",
       "         [0.1039, 0.4748, 0.6943,  ..., 0.5652, 0.6987, 0.2298],\n",
       "         [0.6233, 0.3887, 0.9117,  ..., 0.8732, 0.4357, 0.4482]]),\n",
       " tensor([  1,  42,  13, 204,  23, 697,  31,   4, 261, 481,   2,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0]))"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_size = []\n",
    "sents_length = []\n",
    "for data in ds_iter:\n",
    "    frames_size.append(data[0].shape[0])\n",
    "    sents_length.append(len(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# frames length distribution\n",
    "plt.hist(frames_size)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sentence length distribution\n",
    "plt.hist(sents_length)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6519 491\n"
     ]
    }
   ],
   "source": [
    "# split train/test\n",
    "train_proportion = 0.93\n",
    "train_size = int(train_proportion * len(ds))\n",
    "validation_size = len(ds) - train_size\n",
    "print(train_size, validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, validation_ds = random_split(ds, [train_size, validation_size])\n",
    "# dataloaders\n",
    "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "valid_dl = DataLoader(validation_ds, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 800])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_dl))\n",
    "cnn = nn.Conv1d(40, 10, 5, stride=2)\n",
    "fc = nn.Linear(5110, 800)\n",
    "x = F.max_pool1d(F.relu(cnn(x)), 2)\n",
    "x = torch.flatten(x, 1)\n",
    "x = x.unsqueeze(0)\n",
    "x = fc(x)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to Sequence Model (Encoder-Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While image description handles a variable length output sequence of words, video description also has to handle a variable length input sequence of frames. Related approaches to video description have resolved variable length input by holistic video representations [29, 28, 11], pooling over frames [39], or sub-sampling on a fixed number of input frames [43]. In contrast, in this work we propose a sequence to sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models import resnet18, resnet101\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Conv1d(40, 10, 5, stride=2)\n",
    "        self.fc = nn.Linear(5110, 800)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool1d(F.relu(self.cnn(x)), 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    Decode Hidden State from Encoder to sentence (sequence of texts)\n",
    "    \n",
    "    note: batch_first=True does not apply to hidden or cell states\n",
    "    '''\n",
    "    def __init__(self, weights, emb_dim, hidden_dim, out_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        # layers\n",
    "        self.emb = nn.Embedding.from_pretrained(torch.tensor(weights), padding_idx=0, freeze=False)\n",
    "        self.rnn = nn.GRU(emb_dim + hidden_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.fc_out = nn.Linear(emb_dim + hidden_dim * 2, out_dim)\n",
    "                \n",
    "    def forward(self, word_input, encoded_context, hidden):\n",
    "        '''\n",
    "        word_input: (batch_size)\n",
    "        encoded_context: (1, batch_size, hidden_dim)\n",
    "        hidden: (1, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # 1 word at a time\n",
    "    \n",
    "        word_input = self.emb(word_input) # dim (batch, emb_dim)\n",
    "        emb_input = torch.cat([word_input, encoded_context.squeeze(0)], dim=1)\n",
    "        output, hidden = self.rnn(emb_input.unsqueeze(1).float(), hidden)\n",
    "        prediction = self.fc_out(torch.cat([word_input, encoded_context.squeeze(0), hidden.squeeze(0)], dim=1).float())\n",
    "        return prediction, hidden \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, x, y, teacher_forcing_ratio=0.8):\n",
    "        '''\n",
    "        x: PackedSequence\n",
    "        y: (batch_size, sentence_len(padded))\n",
    "        hidden: (1, batch_size, hidden_dim)\n",
    "        '''\n",
    "        batch_size = y.size(0)\n",
    "        sentence_len = y.size(1)\n",
    "        vocab_size = self.decoder.out_dim\n",
    "        \n",
    "        ##############\n",
    "        # Initialize #\n",
    "        ##############\n",
    "        # tensor for final outputs\n",
    "        outputs = torch.zeros(batch_size, sentence_len, vocab_size).to(self.device)\n",
    "        # last hidden state of the encoder is the context\n",
    "        encoded_context = self.encoder(x) # (1, batch_size, hidden_dim)\n",
    "        # first hidden state \n",
    "        hidden = encoded_context # (1, batch_size, hidden_dim)\n",
    "        # first input '<START>'\n",
    "        word_input = y[:, 0] # (batch_size)\n",
    "        for t in range(1, sentence_len):\n",
    "            #insert input token embedding, previous hidden state and the context state\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden = self.decoder(word_input, encoded_context, hidden)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[:, t, :] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = np.random.rand() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) # dim: (batch_size)\n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            word_input = y[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VOCAB = len(word2idx)\n",
    "EMB_DIM = 300\n",
    "INPUT_DIM = 2048 # resnet50 fc dim\n",
    "HIDDEN_DIM = 800\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = Encoder(INPUT_DIM, HIDDEN_DIM)\n",
    "decoder = Decoder(embedding, EMB_DIM, HIDDEN_DIM, N_VOCAB)\n",
    "model = Seq2Seq(encoder, decoder, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight initialization with N(0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.cnn.weight torch.Size([10, 40, 5])\n",
      "encoder.cnn.bias torch.Size([10])\n",
      "encoder.fc.weight torch.Size([800, 5110])\n",
      "encoder.fc.bias torch.Size([800])\n",
      "decoder.rnn.weight_ih_l0 torch.Size([2400, 1100])\n",
      "decoder.rnn.weight_hh_l0 torch.Size([2400, 800])\n",
      "decoder.rnn.bias_ih_l0 torch.Size([2400])\n",
      "decoder.rnn.bias_hh_l0 torch.Size([2400])\n",
      "decoder.fc_out.weight torch.Size([6157, 1900])\n",
      "decoder.fc_out.bias torch.Size([6157])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name != 'decoder.emb.weight':\n",
    "        print(name, param.shape)\n",
    "        nn.init.normal_(param.data, mean=0, std=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=0)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, threshold=0.001, threshold_mode='rel', min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, lossFun, backwards=True, print_loss=False):\n",
    "    \n",
    "    if backwards == True:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for i, (x, y) in enumerate(tqdm(dataloader)):\n",
    "        \n",
    "        out = model(x, y)\n",
    "        out = out.view(-1, 6157)\n",
    "        y = y.view(-1)\n",
    "        loss = lossFun(out, y)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if i % 50 == 0:\n",
    "            print(f'Batch loss: {loss.item()}')\n",
    "        if backwards == True:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    if print_loss == True:\n",
    "        print(avg_loss)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, lossFun):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for x, y in tqdm(dataloader):\n",
    "\n",
    "            out = model(x, y)\n",
    "            out = out.view(-1, 6157)\n",
    "            y = y.view(-1)\n",
    "            loss = lossFun(out, y)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/815 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "Batch loss: 59.9456901550293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/815 [00:02<39:27,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/815 [00:04<29:02,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/815 [00:05<24:40,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 8, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/815 [00:07<33:42,  2.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-312-ed150cd44f8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Loss: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-311-bd307d21c942>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, lossFun, backwards, print_loss)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbackwards\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train 3 more epochs\n",
    "EPOCHS = 5\n",
    "best = 100000\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('Epoch: ', epoch)\n",
    "    \n",
    "    train_loss = train_one_epoch(model, train_dl, optimizer, loss_func)\n",
    "    print('Train Loss: ', train_loss)\n",
    "    \n",
    "    valid_loss = evaluate(model, valid_dl, loss_func)\n",
    "    print('Valid Loss: ', valid_loss)\n",
    "    \n",
    "    if valid_loss < best:\n",
    "        best = valid_loss\n",
    "        torch.save(model.state_dict(),  '../model/seq2seq_v2.pt')\n",
    "    scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/815 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "encoder output torch.Size([1, 319, 2048])\n",
      "torch.Size([8, 300])\n",
      "torch.Size([1, 319, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): Sizes of tensors must match except in dimension 1. Got 8 and 319 in dimension 0 (The offending index is 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-bb3a67db15e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Loss: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-183-bd307d21c942>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, lossFun, backwards, print_loss)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6157\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-179-4d60f39e9cea>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m#insert input token embedding, previous hidden state and the context state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m#receive output tensor (predictions) and new hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m#place predictions in a tensor holding predictions for each token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-178-cbf21ecbd434>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_input, encoded_context, hidden)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0memb_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): Sizes of tensors must match except in dimension 1. Got 8 and 319 in dimension 0 (The offending index is 1)"
     ]
    }
   ],
   "source": [
    "# train 3 more epochs\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('Epoch: ', epoch)\n",
    "    \n",
    "    train_loss = train_one_epoch(model, train_dl, optimizer, loss_func)\n",
    "    print('Train Loss: ', train_loss)\n",
    "    \n",
    "    valid_loss = evaluate(model, valid_dl, loss_func)\n",
    "    print('Valid Loss: ', valid_loss)\n",
    "    \n",
    "    if valid_loss < best:\n",
    "        best = valid_loss\n",
    "        torch.save(model.state_dict(),  '../model/seq2seq_v2.pt')\n",
    "    scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8.3415e-07,  1.8757e-08, -1.0000e+00, -7.6159e-01,  2.7992e-12,\n",
       "          -3.5686e-05,  8.0224e-15, -1.5713e-10, -1.4366e-13,  3.3050e-13,\n",
       "           2.0661e-21,  1.0000e+00, -1.2542e-10, -4.9077e-11, -9.7051e-05,\n",
       "           1.3208e-20,  7.0467e-07,  1.1476e-12, -1.8057e-11,  8.1142e-15,\n",
       "          -3.8975e-02, -1.4302e-16, -1.0386e-09, -2.6230e-12,  7.6159e-01,\n",
       "          -2.6330e-23, -1.2532e-02, -5.2528e-04, -1.2949e-07,  2.4892e-08,\n",
       "           6.0026e-07, -1.1196e-10,  1.5296e-10, -5.0617e-09,  2.4082e-09,\n",
       "          -3.3455e-08, -1.0477e-16,  4.9381e-16, -9.8111e-22,  1.8580e-14,\n",
       "          -4.9357e-11,  1.0770e-11,  4.9214e-10,  9.3780e-16, -6.4904e-04,\n",
       "           1.0489e-06,  5.5468e-14,  1.2570e-05, -5.2065e-02, -1.0022e-04,\n",
       "          -1.2523e-10, -3.3691e-08,  1.9675e-05, -1.4091e-15,  1.4231e-01,\n",
       "           1.4667e-12,  5.1739e-10, -5.8571e-08, -3.2723e-04,  1.1033e-10,\n",
       "          -3.1379e-28, -2.8651e-10, -1.4404e-19, -9.0455e-08, -7.8566e-11,\n",
       "           1.3765e-10,  4.9446e-08,  2.5476e-09, -3.1349e-08, -2.8717e-14,\n",
       "           4.1009e-12,  9.0636e-10,  1.3285e-11,  4.4793e-11, -5.4704e-17,\n",
       "          -1.9342e-10, -7.6961e-11, -5.8884e-17,  9.2462e-03, -1.0238e-11,\n",
       "          -2.8134e-13,  7.6159e-01,  2.5199e-09, -1.2039e-12, -1.7211e-16,\n",
       "          -1.4748e-07, -8.5063e-12,  4.6814e-16,  1.0759e-16, -1.0000e+00,\n",
       "          -2.6427e-26, -1.0770e-02,  3.1461e-09,  2.5775e-08,  3.7092e-26,\n",
       "           1.6739e-11,  2.0317e-25, -4.5702e-12,  8.0046e-14, -6.0396e-11,\n",
       "           4.7022e-25, -4.7724e-07, -7.5799e-11, -8.4636e-10,  2.0097e-13,\n",
       "           1.0125e-06,  1.7511e-10,  9.7946e-11,  1.1944e-14, -1.0000e+00,\n",
       "           1.0000e+00, -9.7485e-08,  9.7563e-09, -1.0528e-11, -1.0930e-03,\n",
       "           1.8838e-08,  2.3630e-07,  2.1257e-13, -2.5075e-15, -5.2711e-07,\n",
       "           1.3443e-29, -1.3560e-03,  1.3562e-06, -9.9305e-19,  1.0000e+00,\n",
       "           2.6614e-07,  4.3066e-09,  2.4873e-14, -1.4642e-09,  2.3889e-22,\n",
       "           4.3710e-14, -4.6720e-02,  5.6218e-09,  1.5116e-11,  5.6153e-09,\n",
       "           1.0000e+00, -4.6661e-09,  9.7760e-22,  8.9010e-14, -1.6127e-11,\n",
       "          -2.2074e-12,  4.4158e-27, -7.6159e-01, -3.8015e-02,  6.4763e-20,\n",
       "          -6.4404e-08,  9.1023e-09, -1.8285e-13,  9.1019e-12, -1.2197e-08,\n",
       "           2.7767e-10,  6.7147e-10, -9.1421e-09, -5.5355e-08,  3.4110e-11,\n",
       "           2.4689e-12, -2.0813e-07,  2.4052e-14,  2.0371e-09, -8.0117e-24,\n",
       "           1.5338e-21, -4.6145e-03,  1.0000e+00,  6.5855e-11,  6.0851e-11,\n",
       "          -1.2440e-07,  6.6149e-21, -5.2121e-10,  1.0000e+00,  1.6491e-08,\n",
       "          -5.8150e-11, -4.8320e-11,  1.2325e-08, -4.2827e-09,  1.1008e-13,\n",
       "          -1.0000e+00,  5.8404e-11, -3.8083e-10, -1.0000e+00,  6.7080e-11,\n",
       "           2.0297e-10,  1.0270e-01, -3.3564e-13,  1.8523e-15, -1.0000e+00,\n",
       "          -9.4786e-11, -7.0984e-06, -4.0065e-10,  5.5447e-08,  1.0000e+00,\n",
       "           3.0686e-11,  7.9642e-07,  7.0685e-12, -9.5438e-14, -8.2457e-05,\n",
       "          -7.6198e-07, -1.1899e-10, -3.6804e-17,  1.3207e-14, -7.7605e-14,\n",
       "          -9.9476e-01,  5.0469e-06,  4.0656e-16,  3.4020e-15,  1.9151e-06,\n",
       "          -4.0434e-26,  3.2205e-08, -4.3674e-09, -7.6144e-01, -7.3121e-14,\n",
       "          -1.3824e-07,  6.1466e-08, -7.3538e-26,  6.6500e-15, -3.3248e-07,\n",
       "           1.0000e+00,  3.6650e-13, -2.0898e-08,  4.9180e-12,  1.9031e-07,\n",
       "           5.0448e-12, -2.1450e-16, -7.6159e-01,  1.0000e+00,  3.7287e-17,\n",
       "           4.8356e-12,  1.0000e+00, -5.2864e-07, -4.7354e-11,  2.0782e-13,\n",
       "          -1.4318e-08, -9.3999e-11,  2.2857e-04, -1.3566e-10, -3.2166e-03,\n",
       "          -1.9981e-05,  1.4835e-03, -2.7394e-10,  9.2278e-12,  1.7502e-12,\n",
       "          -3.6867e-17, -1.1449e-10,  2.4320e-11, -6.1318e-17, -7.6149e-01,\n",
       "           5.2561e-09, -6.1599e-15, -1.0065e-01,  2.3076e-10,  3.1643e-15,\n",
       "          -5.5232e-01, -1.4257e-06, -2.2189e-13, -2.4821e-06, -5.9745e-14,\n",
       "           1.3974e-04, -5.1463e-22,  7.6159e-01, -2.4480e-03, -3.6327e-11,\n",
       "          -1.4107e-10,  4.0540e-10, -3.3417e-14, -1.3700e-10,  7.6159e-01,\n",
       "          -1.6639e-23,  5.8423e-27,  4.4754e-03, -1.0000e+00, -2.2025e-12,\n",
       "          -1.4419e-13, -7.6159e-01, -1.0093e-07, -4.1858e-17, -4.5342e-14,\n",
       "           1.7498e-10,  2.6384e-11, -4.4152e-09, -4.8146e-15,  2.5242e-06,\n",
       "          -1.9673e-06,  1.0000e+00, -3.9875e-08, -1.5969e-04,  1.5440e-09,\n",
       "          -2.7324e-10,  1.4477e-12, -1.8652e-16,  1.0548e-13, -7.5916e-11,\n",
       "           3.6071e-10,  5.2838e-22, -6.3665e-14, -3.3777e-13, -9.8407e-14,\n",
       "           7.6159e-01,  1.3633e-11,  2.8580e-10,  9.2228e-16, -1.4475e-05,\n",
       "          -1.0058e-09, -5.8477e-07, -6.5089e-09,  4.7209e-09,  2.1132e-16,\n",
       "          -5.9605e-09,  1.4449e-11,  4.6788e-09, -9.3408e-04, -4.8944e-09,\n",
       "          -3.9525e-09, -4.7983e-11, -2.3320e-27,  2.1443e-09,  1.1054e-05,\n",
       "          -1.2043e-11,  2.3734e-08,  3.0749e-12, -2.0329e-09, -4.6275e-02,\n",
       "           1.2237e-06,  1.8574e-10, -1.2889e-17,  7.3204e-15, -7.6159e-01,\n",
       "           1.0000e+00, -8.4847e-13, -7.6159e-01,  4.5644e-12, -4.2921e-11,\n",
       "           4.0110e-08, -6.0500e-07, -1.3565e-08,  2.9219e-09,  4.7153e-11,\n",
       "           1.0000e+00,  6.1012e-14, -9.0220e-01, -1.4923e-16,  6.8048e-10,\n",
       "           3.0781e-17, -1.0078e-13, -1.2629e-09,  2.7254e-12, -8.8841e-11,\n",
       "           9.5877e-17,  3.2370e-10,  1.1602e-10,  5.3071e-11,  2.5113e-10,\n",
       "          -1.0145e-09,  1.9289e-12, -2.2038e-18,  2.2842e-08,  3.9374e-09,\n",
       "          -2.9938e-08,  1.4297e-30, -1.6779e-18, -3.4992e-11,  1.2163e-09,\n",
       "           8.7093e-08,  2.6112e-12,  3.6172e-12, -1.0756e-09,  3.4260e-09,\n",
       "          -7.5577e-09, -5.0272e-10,  8.0846e-11,  4.5662e-14,  1.6713e-09,\n",
       "          -3.0490e-05,  1.3711e-13, -5.4495e-10,  3.5678e-10,  2.5634e-11,\n",
       "           4.7728e-11,  1.5807e-08,  3.8139e-10,  2.0863e-11,  1.1218e-09,\n",
       "           1.1221e-01,  6.5520e-26,  7.6177e-10,  4.2122e-11, -1.5374e-03,\n",
       "          -2.8512e-11,  3.7352e-02,  1.0000e+00, -1.0000e+00, -1.0765e-06,\n",
       "           1.0555e-15,  3.1016e-11, -1.2957e-09, -1.1898e-07,  1.0932e-10,\n",
       "          -9.6253e-09, -2.0374e-24,  8.0792e-07,  1.1558e-09,  9.9283e-07,\n",
       "          -1.1455e-13,  7.5371e-09, -2.3519e-09, -1.3080e-10,  1.6785e-06,\n",
       "          -7.4822e-13,  7.0594e-15,  1.0701e-12,  6.1955e-15,  3.2217e-06,\n",
       "          -2.2987e-06, -3.0029e-20,  1.3897e-10,  9.9999e-01, -6.0170e-16,\n",
       "           4.1747e-04,  1.4579e-12, -6.2663e-16,  4.6652e-11,  6.8469e-10,\n",
       "           1.0785e-07, -3.0025e-15,  1.0000e+00,  1.1956e-20, -1.0000e+00,\n",
       "           6.1595e-07, -9.6382e-05, -3.9603e-11,  7.6159e-01, -2.7042e-28,\n",
       "           9.4996e-13, -3.4837e-09, -1.4055e-06, -2.9560e-06, -2.3441e-08,\n",
       "           1.7434e-01,  3.1450e-07,  6.9858e-10,  2.2650e-06,  7.5632e-01,\n",
       "          -8.5240e-25,  2.4636e-10, -1.6547e-16, -4.5360e-11,  1.3631e-18,\n",
       "           1.7080e-14,  5.2297e-08,  2.6665e-11, -2.1053e-05,  6.0264e-14,\n",
       "          -5.2385e-15, -1.1067e-07, -1.5459e-12, -9.6062e-12,  1.6440e-10,\n",
       "          -1.9503e-13,  1.7698e-09,  7.6160e-01,  1.4645e-05, -1.9994e-12,\n",
       "          -5.4941e-20, -1.9083e-11, -3.0622e-08, -1.2029e-07,  1.4261e-04,\n",
       "           3.0314e-09,  4.9384e-08, -1.3734e-05,  1.0000e+00,  1.4927e-08,\n",
       "          -5.2514e-17,  2.5863e-10, -1.0000e+00,  2.6630e-09,  6.2905e-03,\n",
       "           3.6822e-15,  2.3792e-13,  9.6616e-20,  7.6159e-01, -4.2835e-10,\n",
       "           4.8274e-13, -1.8301e-15,  2.7109e-12, -4.2892e-24, -2.3211e-08,\n",
       "           3.8126e-09, -1.3649e-06,  3.7600e-12, -4.2794e-13, -3.1722e-25,\n",
       "          -5.5280e-20,  4.8895e-08,  1.4968e-09, -5.0621e-09, -7.8561e-14,\n",
       "          -3.6016e-15, -2.9086e-12, -1.0316e-03,  4.0109e-10, -2.1743e-02,\n",
       "          -1.6683e-01,  3.1676e-33, -3.1692e-19,  7.8820e-06, -9.3719e-13,\n",
       "          -1.8513e-09,  1.1684e-16, -3.9326e-09,  1.3238e-07, -1.0000e+00,\n",
       "           3.3547e-11, -4.6864e-14,  1.0101e-20, -6.4408e-20,  1.1804e-07,\n",
       "           5.1590e-07,  1.2512e-09, -2.5929e-09, -3.6057e-05,  4.7752e-15,\n",
       "          -8.1391e-07, -2.8379e-08,  5.3851e-11, -1.0000e+00, -4.3296e-08,\n",
       "          -1.3523e-11, -1.9586e-07, -4.6160e-10, -4.8485e-15, -1.5846e-08,\n",
       "          -2.9453e-24, -7.8629e-10, -5.8177e-06, -1.5854e-11,  4.1974e-07,\n",
       "           2.8464e-12,  4.6945e-08,  9.1518e-09, -1.4985e-16,  7.0716e-02,\n",
       "           6.3613e-08,  1.0000e+00, -3.9775e-10,  7.6159e-01,  1.3115e-35,\n",
       "          -1.3815e-09,  2.8125e-08,  1.4548e-02, -1.7705e-12,  1.2035e-15,\n",
       "          -1.6492e-02, -5.3461e-09,  5.6090e-16, -4.9492e-09, -2.1196e-04,\n",
       "           1.3934e-11,  3.9153e-07, -1.8111e-11, -9.8962e-05,  8.9453e-06,\n",
       "           2.9542e-14,  3.3710e-08,  9.0423e-10,  2.4779e-27, -4.8405e-21,\n",
       "           1.0000e+00, -7.2679e-07, -1.0000e+00,  1.6877e-11,  1.2025e-07,\n",
       "           1.8433e-18, -6.0712e-11, -7.4386e-11,  2.2427e-09, -4.6506e-10,\n",
       "           1.2461e-09,  2.6077e-05,  1.0074e-11,  5.0857e-11, -2.6322e-09,\n",
       "           2.5371e-15,  2.9549e-11,  1.9869e-15,  2.4526e-12, -1.0000e+00,\n",
       "          -1.5903e-11, -3.0295e-04, -4.4557e-16,  2.0546e-11,  4.8306e-22,\n",
       "           2.9260e-11, -9.3240e-21, -2.0892e-10,  3.1496e-09,  1.9104e-16,\n",
       "          -9.0668e-12, -2.1620e-10,  4.5663e-15,  1.0000e+00, -1.8464e-12,\n",
       "           1.2578e-07, -6.4666e-13,  1.5219e-10,  3.8444e-08,  1.4972e-17,\n",
       "          -1.9575e-10, -1.8429e-13,  1.2215e-14,  1.5118e-08, -6.5224e-08,\n",
       "           3.5603e-08, -2.4907e-10, -6.6299e-08, -4.0116e-09,  1.0257e-11,\n",
       "           2.7645e-08, -7.6159e-01, -3.1893e-11,  4.6584e-06, -1.0786e-11,\n",
       "          -1.0752e-10,  1.4273e-11, -1.2721e-09, -5.5312e-10,  1.1148e-02,\n",
       "          -9.7147e-15,  8.5599e-11,  4.4218e-09, -9.5257e-31, -9.8891e-09,\n",
       "          -1.0011e-05,  2.8926e-08,  6.4256e-16,  1.9018e-13, -1.9039e-22,\n",
       "           1.5072e-05,  1.0647e-08,  1.2581e-12, -1.2566e-09,  7.3255e-07,\n",
       "          -1.7681e-10, -9.9940e-10,  6.7118e-18,  6.5256e-09, -2.4323e-15,\n",
       "           1.3004e-08,  8.3823e-10, -7.9870e-10, -4.4808e-14,  2.8769e-11,\n",
       "           1.4666e-12, -4.6944e-10,  1.3147e-08,  5.2438e-20,  4.9397e-11,\n",
       "          -1.0000e+00, -3.2699e-09,  6.1717e-11, -1.6918e-09, -2.7133e-12,\n",
       "           3.1078e-10, -1.4057e-10, -1.0000e+00, -3.0822e-06,  4.9731e-08,\n",
       "           1.2374e-12,  2.1570e-08,  2.8260e-18, -3.4944e-08, -8.2411e-04,\n",
       "          -4.0212e-12, -3.5951e-02,  9.6121e-18, -2.7090e-06,  1.0838e-08,\n",
       "           8.4866e-09, -3.2158e-03,  1.3993e-03,  7.0009e-21, -7.1436e-12,\n",
       "           6.2904e-09, -7.6159e-01,  1.5544e-12, -8.1295e-09,  2.5074e-12,\n",
       "          -2.9300e-04, -2.6335e-29,  1.5366e-12, -3.7797e-10,  2.3093e-11,\n",
       "          -1.2398e-12,  8.9382e-11,  5.4210e-12,  3.8848e-09, -6.7352e-13,\n",
       "          -7.7398e-11,  2.8760e-35,  2.0964e-21,  1.5266e-14, -8.7174e-08,\n",
       "          -4.7285e-10,  2.6316e-14,  1.0349e-06, -4.9397e-04, -2.0679e-02,\n",
       "          -3.7752e-17, -4.0779e-08, -6.3816e-13,  2.4583e-09,  3.1125e-18,\n",
       "           1.0707e-07,  2.2753e-10, -8.8847e-07,  5.4379e-13, -5.1292e-03,\n",
       "          -4.0126e-15,  1.7044e-09,  8.9347e-09, -1.3989e-09,  2.7211e-10,\n",
       "           4.1499e-03, -8.2397e-01, -3.0989e-02, -4.2240e-11, -6.9282e-07,\n",
       "           1.9060e-14,  2.1969e-12,  3.5942e-09,  1.0000e+00, -1.8007e-08,\n",
       "           1.3574e-14,  1.7919e-10, -1.2696e-16,  5.6128e-11,  1.7713e-07,\n",
       "           2.0617e-14,  1.8149e-02, -1.6247e-07, -2.7812e-10, -1.0601e-09,\n",
       "           1.3909e-09,  2.1682e-08,  2.7390e-07, -1.1030e-14,  5.7847e-08,\n",
       "           2.4637e-03,  7.0279e-10, -4.0922e-12,  1.5467e-11,  3.6512e-07,\n",
       "           6.0392e-09, -1.3410e-10, -1.8193e-10, -1.8911e-06,  1.3519e-10,\n",
       "           1.0196e-21,  1.2184e-11,  6.2724e-12,  1.2503e-14,  1.8700e-07,\n",
       "          -6.2604e-12,  1.0000e+00,  4.4132e-09, -6.5667e-10, -1.8251e-09,\n",
       "           3.2559e-17,  1.3010e-09,  6.8135e-20,  2.6176e-10,  7.6159e-01,\n",
       "           6.7349e-25,  7.6159e-01, -5.1687e-10, -2.1046e-11,  2.1768e-18,\n",
       "          -7.4948e-02,  1.6387e-09, -2.5176e-09, -3.3847e-13, -4.1844e-13,\n",
       "          -9.3688e-10, -3.6494e-10,  1.0000e+00, -6.6247e-07, -2.4310e-07,\n",
       "           5.6255e-25,  4.0146e-13, -2.8625e-19,  1.5358e-07, -1.1666e-07,\n",
       "           1.2818e-08,  3.4116e-09, -9.8157e-12, -9.8172e-10, -6.0956e-19,\n",
       "          -3.7073e-24,  6.1957e-12,  1.0106e-16,  2.8816e-11,  2.2423e-24]]])"
      ]
     },
     "execution_count": 1058,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    context = model.encoder(test_x.unsqueeze(0))\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_seq(model, x, start_token, seq_length):\n",
    "    with torch.no_grad():\n",
    "        word_input = torch.tensor(word2idx[start_token]).unsqueeze(0)\n",
    "        context = model.encoder(x.unsqueeze(0))\n",
    "        hidden = context\n",
    "        outputs = [start_token]\n",
    "        ## generate a sequence!\n",
    "        for i in range(seq_length):\n",
    "            output, hidden = model.decoder(word_input, context, hidden)\n",
    "            word = output.argmax(1)\n",
    "            outputs.append(idx2word[word.item()])\n",
    "            word_input = word\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 2048])"
      ]
     },
     "execution_count": 1076,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x, test_y = next(iterator)\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,    4, 1082,   25, 5597,  104,   25,  219,    5, 2308,  305,   12,\n",
       "           4,  389,    2])"
      ]
     },
     "execution_count": 1081,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START>',\n",
       " 'a',\n",
       " 'teenage',\n",
       " 'girl',\n",
       " 'teases',\n",
       " 'another',\n",
       " 'girl',\n",
       " 'who',\n",
       " 'is',\n",
       " 'blindfolded',\n",
       " 'next',\n",
       " 'to',\n",
       " 'a',\n",
       " 'river',\n",
       " '<END>']"
      ]
     },
     "execution_count": 1082,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx2word[word.item()] for word in test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4117e-07,  8.7523e-09, -1.0000e+00, -7.6159e-01,  3.1522e-14,\n",
      "         -3.8256e-06,  1.7946e-15, -1.5878e-11, -6.9432e-16,  2.3584e-15,\n",
      "          1.6964e-22,  1.0000e+00, -2.9911e-12, -1.3260e-12, -6.8926e-03,\n",
      "          1.2900e-22,  3.9233e-08,  1.9082e-13, -6.0221e-12,  3.7145e-15,\n",
      "         -6.2369e-03, -7.8452e-15, -9.1389e-10, -1.4576e-13,  7.6159e-01,\n",
      "         -3.6218e-25, -4.9680e-03, -2.1962e-04, -1.4119e-09, -1.8743e-08,\n",
      "          6.3280e-08, -1.4303e-11,  1.7118e-11, -1.7941e-09,  1.9121e-10,\n",
      "         -2.3796e-08, -3.5009e-14,  7.3931e-16, -4.8089e-22,  2.3265e-13,\n",
      "         -8.8118e-11,  5.9196e-12,  6.9587e-11,  5.1894e-16, -4.3336e-06,\n",
      "          1.3440e-05,  2.3725e-14,  3.2155e-04, -4.9125e-01, -4.2814e-04,\n",
      "         -1.0928e-10, -4.0647e-09,  1.0124e-05, -4.2381e-17,  6.6224e-02,\n",
      "          1.7039e-13,  4.5293e-11, -2.1232e-09, -5.8124e-06,  2.0353e-11,\n",
      "         -5.4074e-28, -4.1671e-11,  9.0823e-21, -4.6344e-07, -4.0680e-09,\n",
      "          1.1559e-11,  4.7354e-08,  2.8662e-10, -3.7436e-08, -3.7561e-15,\n",
      "          6.2532e-13,  2.0632e-09,  3.9544e-10,  2.5569e-13, -6.9883e-17,\n",
      "         -3.0408e-11, -3.3246e-12, -2.9005e-17,  6.2029e-02, -1.8190e-10,\n",
      "         -3.6354e-14,  7.6149e-01,  4.2224e-10,  1.5928e-14, -6.9256e-17,\n",
      "         -5.4321e-08, -9.6946e-13,  9.4571e-16,  7.4187e-17, -1.0000e+00,\n",
      "         -1.1590e-25, -1.0380e-02,  3.0847e-10,  6.9025e-10,  4.6102e-27,\n",
      "          1.4606e-12,  1.1103e-27, -7.4015e-14,  1.4983e-15, -2.6031e-14,\n",
      "          1.9532e-27, -2.1862e-06, -5.3637e-15, -1.9830e-09,  5.0210e-14,\n",
      "          6.4224e-08,  2.2588e-10,  1.4480e-11,  5.1450e-15, -1.0000e+00,\n",
      "          1.0000e+00, -5.7192e-08,  6.2660e-10, -1.1870e-11, -1.6119e-10,\n",
      "          1.1713e-08,  8.9438e-09,  1.1683e-14, -3.0721e-17, -6.7020e-07,\n",
      "          5.0476e-28, -4.4026e-04,  1.6557e-06, -1.4472e-17,  1.0000e+00,\n",
      "          9.5120e-08,  2.8691e-11,  6.0137e-16, -1.5287e-08,  3.0611e-24,\n",
      "          1.5935e-12, -1.1986e-02,  7.2924e-10,  8.0699e-12,  1.8389e-08,\n",
      "          1.0000e+00, -2.0764e-07,  1.5540e-22,  2.7722e-14, -4.9706e-12,\n",
      "         -7.2602e-11,  6.5146e-28, -7.6159e-01, -1.0557e-01,  1.6052e-18,\n",
      "         -2.5803e-07,  1.0888e-09, -5.7935e-13,  2.1470e-10, -4.5960e-09,\n",
      "          1.8827e-09,  6.4487e-12, -9.8835e-11, -2.5798e-09,  2.0480e-12,\n",
      "          3.6949e-12, -1.5605e-07,  5.4132e-16,  1.9886e-10, -3.9069e-23,\n",
      "          2.5015e-20, -3.0370e-02,  1.0000e+00,  7.1888e-11,  3.4571e-11,\n",
      "         -5.4059e-08,  1.6359e-20, -4.5683e-09,  9.9999e-01,  1.7351e-10,\n",
      "         -1.1338e-10, -3.9823e-11,  2.6220e-09, -1.3732e-10,  8.7649e-15,\n",
      "         -1.0000e+00,  4.7112e-11,  4.6365e-09, -1.0000e+00,  2.4144e-10,\n",
      "          7.5875e-12,  1.5687e-02, -7.6119e-14,  2.4740e-16, -1.0000e+00,\n",
      "         -5.7678e-10, -7.3192e-06, -6.0900e-12,  2.9233e-08,  1.0000e+00,\n",
      "          6.9372e-11,  9.0171e-07,  2.1617e-12, -1.7255e-15, -8.6092e-06,\n",
      "         -4.0599e-05, -1.8790e-10, -9.7500e-17,  9.0255e-15, -9.6399e-14,\n",
      "         -9.9726e-01,  2.4485e-07,  2.1082e-17,  5.7377e-15,  6.6742e-07,\n",
      "         -5.7819e-26,  2.1302e-09, -2.9053e-10, -7.6146e-01, -5.2360e-15,\n",
      "         -2.0819e-08,  3.4933e-07, -6.2427e-25,  1.3368e-16, -2.5033e-07,\n",
      "          1.0000e+00,  1.5628e-14, -8.2989e-08,  7.4026e-11,  1.5359e-07,\n",
      "          9.5360e-11, -1.4193e-14, -7.6159e-01,  1.0000e+00,  9.1913e-17,\n",
      "          2.5102e-12,  1.0000e+00, -3.0106e-06, -7.6827e-13,  1.7749e-12,\n",
      "         -1.7295e-09, -2.5308e-11,  2.2078e-04, -3.3322e-12, -7.4892e-05,\n",
      "         -1.8172e-10,  7.7724e-03, -5.9775e-09,  3.1077e-16,  2.3391e-13,\n",
      "         -1.6829e-15, -1.4821e-11,  2.8355e-12, -2.3407e-17, -7.6149e-01,\n",
      "          1.8617e-09, -3.0505e-16, -3.8228e-04,  1.0844e-10,  5.4246e-14,\n",
      "         -2.6865e-05,  6.3427e-09, -3.0115e-15, -1.7472e-08, -3.5526e-15,\n",
      "          1.0696e-03, -1.9761e-22,  7.6159e-01, -8.4805e-03, -2.6167e-11,\n",
      "         -7.2835e-11,  8.1107e-10, -1.2330e-16, -4.1332e-11,  7.6159e-01,\n",
      "          2.3867e-21,  8.6312e-26,  1.3705e-02, -1.0000e+00, -5.6145e-14,\n",
      "         -1.1421e-13, -7.6159e-01, -3.0273e-08, -1.1595e-17, -5.9498e-14,\n",
      "          6.9987e-11,  1.2391e-12, -3.5926e-09, -2.5420e-17,  8.8128e-07,\n",
      "         -4.7335e-05,  1.0000e+00, -4.4238e-09, -4.1015e-05,  2.8415e-10,\n",
      "         -1.0095e-11,  3.5823e-13, -3.3867e-16,  1.0297e-15, -5.1245e-12,\n",
      "          1.8959e-09,  1.0820e-27, -1.5409e-14, -7.7473e-14, -4.6230e-14,\n",
      "          7.6159e-01,  4.7449e-10,  1.0959e-09,  3.3426e-16, -2.2245e-02,\n",
      "         -1.7266e-08, -1.1263e-06, -6.9595e-09,  7.8378e-10,  1.0415e-17,\n",
      "         -1.0256e-09,  2.8934e-13,  3.3835e-09, -3.1443e-03, -2.5775e-10,\n",
      "         -1.9229e-08, -2.1684e-10, -2.7806e-29,  2.0334e-10,  8.1621e-09,\n",
      "         -2.3713e-12,  1.3256e-08,  1.4616e-12, -5.5373e-09, -1.0774e-01,\n",
      "          2.1373e-06,  3.7278e-11, -7.7566e-18,  1.9618e-13, -7.6159e-01,\n",
      "          1.0000e+00, -2.4397e-13, -7.6159e-01,  7.4537e-14, -2.6240e-11,\n",
      "          1.5787e-08, -7.3339e-07, -1.1540e-08,  1.9391e-08,  7.7935e-10,\n",
      "          1.0000e+00,  4.2305e-13, -9.9257e-01, -3.5747e-16,  2.3759e-10,\n",
      "          1.4193e-18, -1.2610e-13, -7.4174e-09,  4.3817e-13, -1.1871e-10,\n",
      "          8.6491e-16,  4.4588e-10, -2.0193e-10,  5.8526e-13,  7.2654e-11,\n",
      "         -7.2221e-11,  2.4813e-16,  9.2645e-18,  3.3722e-08,  3.9729e-11,\n",
      "         -4.1256e-08,  1.6494e-30, -3.2448e-19, -7.3115e-13,  5.3978e-10,\n",
      "          2.6623e-09,  2.6309e-12,  1.1344e-13, -4.0422e-08,  1.9503e-09,\n",
      "         -3.5239e-13, -5.8951e-09,  7.4276e-10,  1.5624e-12,  5.2836e-11,\n",
      "         -1.4405e-05,  1.1782e-13, -7.4189e-10,  6.1547e-10,  6.2220e-11,\n",
      "          2.0195e-11,  4.9730e-08,  4.2716e-10,  3.0944e-10,  1.1339e-10,\n",
      "          2.3477e-03,  3.9415e-26,  7.5653e-11,  1.5130e-11, -5.5048e-04,\n",
      "         -4.7897e-11,  4.0087e-02,  1.0000e+00, -1.0000e+00, -1.5208e-07,\n",
      "          1.6419e-16,  2.6392e-10, -2.3713e-09, -2.3509e-08,  4.8261e-12,\n",
      "         -3.7136e-09, -5.2525e-25,  6.4306e-06,  1.1439e-10,  3.3300e-06,\n",
      "         -7.6633e-13,  8.8600e-11, -1.8566e-09, -4.2415e-10,  2.5614e-06,\n",
      "         -3.1022e-12,  1.9882e-15,  1.1271e-13,  5.6278e-15,  4.7332e-05,\n",
      "         -1.4750e-06, -2.6308e-32,  1.2399e-10,  1.0000e+00, -3.6163e-17,\n",
      "          3.6886e-03,  1.6916e-13, -3.8392e-16,  1.1310e-10,  3.2605e-08,\n",
      "          8.8740e-09, -1.8178e-14,  1.0000e+00,  3.5008e-20, -1.0000e+00,\n",
      "          3.1274e-08, -7.3540e-05, -5.1719e-12,  7.6159e-01, -3.4652e-28,\n",
      "          1.6255e-13, -6.3225e-09, -6.2576e-07, -1.3482e-05, -6.2789e-09,\n",
      "          2.8319e-02,  8.0097e-07,  2.1955e-10,  1.5472e-05,  6.0343e-01,\n",
      "         -1.9646e-27,  5.5687e-10, -1.6656e-17, -2.1939e-10,  1.6629e-19,\n",
      "          1.1384e-12,  7.1345e-08,  1.8387e-11, -6.7678e-04,  1.7721e-15,\n",
      "         -8.6402e-16, -1.1080e-07, -1.0582e-12, -3.9951e-14,  7.2695e-12,\n",
      "         -8.5448e-14,  1.7687e-10,  7.6159e-01,  3.1165e-06, -3.3038e-11,\n",
      "         -4.5536e-22, -2.8361e-10, -4.6749e-09, -8.7432e-09,  5.8803e-06,\n",
      "          4.6824e-10,  8.3651e-07, -3.1462e-05,  1.0000e+00,  3.3722e-08,\n",
      "         -2.9231e-22,  1.1097e-09, -1.0000e+00,  2.8563e-08,  1.3241e-02,\n",
      "          7.3233e-17,  3.1056e-13,  3.5401e-19,  7.6159e-01, -2.4339e-10,\n",
      "          1.2650e-14, -4.1841e-15,  1.0073e-13, -6.4758e-24, -1.1075e-09,\n",
      "          9.2587e-10, -6.0008e-07,  4.0688e-13, -1.8207e-13, -4.8923e-26,\n",
      "         -1.7420e-21,  8.7554e-09,  1.1376e-08, -4.4900e-10, -9.1370e-15,\n",
      "         -1.7988e-16, -2.6569e-12, -6.8553e-03,  1.1413e-10, -7.9636e-04,\n",
      "         -6.5949e-02,  1.5679e-32, -1.2419e-19,  1.0925e-04, -1.7770e-13,\n",
      "         -1.7137e-12,  9.4637e-18, -1.0125e-09,  1.7198e-07, -1.0000e+00,\n",
      "          3.2412e-11, -4.1830e-15,  3.8969e-23, -1.5182e-21,  1.9724e-09,\n",
      "          9.3991e-08,  1.4568e-09, -3.1820e-10, -3.3634e-08,  1.3525e-14,\n",
      "         -3.4539e-07, -2.4906e-09,  4.0240e-10, -1.0000e+00, -5.5455e-07,\n",
      "         -3.5129e-12, -3.0837e-07, -3.0624e-11, -9.3472e-16, -3.6397e-08,\n",
      "         -2.0868e-26, -2.1006e-09, -1.2685e-06, -5.2853e-11,  5.8995e-07,\n",
      "          5.9547e-11,  6.1866e-07,  2.3210e-09, -4.9252e-17,  4.9831e-02,\n",
      "          1.5727e-08,  1.0000e+00, -2.7648e-11,  7.6159e-01,  1.5059e-34,\n",
      "         -2.7834e-10,  1.4460e-10,  3.6196e-02, -4.4698e-13,  3.9453e-15,\n",
      "         -8.5393e-02, -1.6842e-07,  1.1232e-15, -1.4696e-09, -4.7414e-04,\n",
      "          1.5349e-10,  4.8244e-08, -3.4383e-12, -1.4993e-03,  7.9052e-06,\n",
      "          3.7914e-13,  9.1726e-09,  9.4922e-10,  4.6765e-27, -4.0832e-30,\n",
      "          1.0000e+00, -1.7869e-07, -1.0000e+00,  1.3999e-12,  3.1964e-08,\n",
      "          8.8748e-20, -6.3452e-12, -1.0730e-12,  1.6067e-09, -9.1809e-11,\n",
      "          6.9910e-09,  3.5725e-06,  8.0569e-11,  1.3688e-09, -1.8264e-09,\n",
      "          1.4886e-17,  3.0218e-12,  1.9658e-16,  1.9727e-12, -1.0000e+00,\n",
      "         -3.3710e-12, -7.6421e-03, -4.2873e-19,  1.6984e-11,  5.5680e-22,\n",
      "          3.4380e-11, -1.9649e-19,  3.7844e-10,  3.0502e-09,  2.8270e-17,\n",
      "         -9.4595e-12, -3.4353e-10,  1.4555e-16,  1.0000e+00, -7.8783e-13,\n",
      "          1.3530e-07, -1.3736e-13,  7.5945e-11,  3.7615e-08,  1.8348e-17,\n",
      "         -1.2461e-12, -1.0916e-13,  1.4398e-16,  8.9054e-10, -5.9840e-11,\n",
      "          6.2739e-08, -2.0034e-13, -4.9394e-08, -1.2011e-10,  5.6463e-15,\n",
      "          6.9865e-09, -7.6159e-01, -5.1026e-13,  1.0399e-06, -1.4847e-10,\n",
      "         -5.3218e-11,  7.5937e-12, -1.4409e-10, -2.4531e-09,  2.3266e-02,\n",
      "         -1.3769e-14,  5.3369e-12,  2.1179e-09, -1.8869e-32, -1.4122e-10,\n",
      "         -5.8087e-08,  3.1744e-08, -5.8721e-18,  1.9446e-12, -1.2793e-22,\n",
      "          1.4774e-05,  9.7416e-09,  9.9138e-13, -9.0729e-10,  1.9945e-07,\n",
      "         -1.0438e-09, -4.4558e-11,  6.3284e-19,  2.1622e-09, -3.3039e-17,\n",
      "          1.6191e-09,  5.3645e-08,  4.4746e-09, -8.4829e-15,  2.7593e-10,\n",
      "          6.1674e-12, -2.8141e-09,  7.9385e-10,  1.3497e-19,  2.9746e-12,\n",
      "         -1.0000e+00, -1.8682e-09,  1.6333e-11, -9.3504e-11, -9.9482e-13,\n",
      "          3.5098e-09, -3.9344e-10, -1.0000e+00, -2.9784e-05,  2.2254e-07,\n",
      "          7.4935e-13,  2.5343e-09,  3.8036e-19, -7.5648e-08, -2.6596e-03,\n",
      "         -1.0301e-12, -1.0676e-01,  1.0790e-17, -2.3080e-05,  3.0900e-09,\n",
      "          2.0995e-07, -1.6127e-02,  1.9554e-03,  1.2966e-22, -2.2634e-11,\n",
      "          1.0531e-08, -7.6159e-01,  9.2592e-13, -9.7153e-10,  8.0327e-13,\n",
      "         -3.7925e-05, -1.1155e-28,  3.0986e-13, -6.9145e-10,  2.7848e-12,\n",
      "         -2.2563e-11,  1.3677e-12,  3.6969e-13,  3.1914e-09, -4.8573e-14,\n",
      "         -1.6780e-12,  2.6248e-36,  1.0008e-19,  2.0923e-14, -2.2811e-08,\n",
      "         -3.0926e-10,  3.9965e-14,  2.6131e-09, -6.4564e-04, -4.6896e-03,\n",
      "         -2.1446e-20, -1.1155e-09, -6.7710e-15,  1.8580e-08,  5.8405e-18,\n",
      "          2.2210e-07,  7.1139e-10, -1.2789e-06,  2.4396e-13, -7.5539e-03,\n",
      "         -1.3501e-14,  8.1098e-11,  3.4330e-08, -4.5698e-10,  7.3837e-12,\n",
      "          7.3671e-03, -9.7660e-01, -1.2524e-01, -6.5852e-13, -9.7861e-07,\n",
      "          1.5530e-12,  5.9318e-14,  1.0042e-07,  1.0000e+00, -5.4036e-08,\n",
      "          8.1218e-14,  1.0062e-10, -2.2571e-17,  1.2942e-11,  4.6399e-09,\n",
      "          1.6428e-15,  3.0889e-03, -1.0591e-06, -3.3350e-11, -1.1716e-08,\n",
      "          4.7801e-11,  1.2833e-10,  3.8974e-06, -2.0278e-15,  3.9878e-08,\n",
      "          2.1680e-02,  4.2529e-11, -2.8248e-12,  2.7899e-11,  3.7562e-07,\n",
      "          1.8168e-09, -8.8185e-11, -5.8098e-11, -1.6056e-07,  1.2470e-10,\n",
      "          1.0103e-22,  6.6172e-13,  2.1675e-12,  3.6789e-19,  1.0621e-13,\n",
      "         -1.4468e-10,  1.0000e+00,  1.2587e-08, -2.0389e-09, -2.9217e-10,\n",
      "          5.4311e-18,  1.0781e-08,  1.6350e-22,  1.4750e-09,  7.6159e-01,\n",
      "          1.8868e-24,  7.6159e-01, -2.0763e-09, -3.8427e-11,  7.6024e-19,\n",
      "         -1.4530e-01,  3.1491e-10, -9.9076e-11, -1.0852e-12, -4.4555e-14,\n",
      "         -1.6537e-08, -1.7832e-11,  1.0000e+00, -4.2561e-06, -1.7188e-07,\n",
      "          2.9461e-24,  6.5546e-14, -2.9698e-21,  7.6590e-07, -1.7653e-09,\n",
      "          1.4058e-08,  1.8002e-10,  2.0840e-12, -1.1256e-09,  1.1561e-19,\n",
      "         -4.0458e-23,  1.8863e-10,  1.4637e-17,  1.9066e-10,  3.2445e-28]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<START>',\n",
       " 'a',\n",
       " 'man',\n",
       " 'is',\n",
       " 'talking',\n",
       " 'about',\n",
       " 'a',\n",
       " 'car',\n",
       " '<END>',\n",
       " '<END>',\n",
       " 'his',\n",
       " 'father',\n",
       " 'in',\n",
       " 'a',\n",
       " 'home',\n",
       " '<END>']"
      ]
     },
     "execution_count": 1078,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_seq(model, test_x,  '<START>', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
